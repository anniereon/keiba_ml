{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af6956c0-b967-49a4-a71d-69829a0c2d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å…¨ãƒ‡ãƒ¼ã‚¿: 634887ä»¶\n",
      "ğŸ‡ éå»å®Ÿç¸¾ãƒ™ãƒ¼ã‚¹ã®è„šè³ªã‚’ç®—å‡ºä¸­...\n",
      "æ¬ æå‡¦ç†ä¸­...\n",
      "å‡¦ç†å¾Œ: 24604ä»¶ï¼ˆå…ƒ:39044ï¼‰\n",
      "ğŸ‡ å±•é–‹äºˆæƒ³ãƒ™ãƒ¼ã‚¹ã®è„šè³ªã‚’ç®—å‡ºä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryo\\AppData\\Local\\Temp\\ipykernel_7004\\2225224117.py:145: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_target[\"running_style_type_pred\"] = df_target.groupby(\"race_id\", group_keys=False).apply(predict_front_back)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¢ ä»Šå›ã®å‡¦ç†ã§ã¯ ã€running_style_type_predã€‘ ã‚’åŸºæº–ã«å‹ç‡ãƒ»å±•é–‹ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
      "âœ… ç‰¹å¾´é‡ä½œæˆå®Œäº†\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "import japanize_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ===============================================================\n",
    "# â˜…â˜…â˜… ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šå¤‰æ•° â˜…â˜…â˜…\n",
    "# ===============================================================\n",
    "GROUP_BY_COURSE = False\n",
    "GROUP_BY_NUM_HORSES = True\n",
    "\n",
    "# --- è„šè³ªåˆ¤å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠè¨­å®š ---\n",
    "# \"past\": éå»ã®é€šéé †ä½ç‡ã®å¹³å‡å€¤ã‚’ä½¿ç”¨\n",
    "# \"pred\": ä»Šå›ã®æˆ¦æ³•å(style_name)ã¨æŒ‡æ•°å·®ã«ã‚ˆã‚‹å±•é–‹äºˆæƒ³ã‚’ä½¿ç”¨\n",
    "STYLE_MODE = \"pred\" \n",
    "\n",
    "# ===============================================================\n",
    "# é–¢æ•°å®šç¾©\n",
    "# ===============================================================\n",
    "def confidence_interval(mean: float, std: float, n: int, confidence: float = 0.95):\n",
    "    t_value = stats.t.ppf((1 + confidence) / 2, df=n-1)\n",
    "    margin = t_value * std / np.sqrt(n)\n",
    "    return mean - margin, mean + margin\n",
    "\n",
    "def handle_missing_val(df, features, enabled, min_horses):\n",
    "    if not enabled:\n",
    "        print(\"âš  æ¬ æå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—\")\n",
    "        return df\n",
    "\n",
    "    print(\"æ¬ æå‡¦ç†ä¸­...\")\n",
    "    processed_groups = []\n",
    "    for race_id, group in df.groupby(\"race_id\"):\n",
    "        actual_min_horses = group[\"num_horses\"].iloc[0]\n",
    "        non_missing_mask = group[features].notnull().all(axis=1)\n",
    "        if non_missing_mask.sum() < actual_min_horses: \n",
    "            continue\n",
    "        for col in features:\n",
    "            if group[col].isnull().any():\n",
    "                group[col] = group[col].fillna(group[col].mean(skipna=True))\n",
    "        processed_groups.append(group)\n",
    "    df_processed = pd.concat(processed_groups, ignore_index=True)\n",
    "    print(f\"å‡¦ç†å¾Œ: {len(df_processed)}ä»¶ï¼ˆå…ƒ:{len(df)}ï¼‰\")\n",
    "    return df_processed\n",
    "\n",
    "def get_last_corner_position(row):\n",
    "    for i in [4, 3, 2, 1]:\n",
    "        val = row.get(f\"position_{i}\")\n",
    "        if pd.notnull(val):\n",
    "            return val\n",
    "    return np.nan\n",
    "\n",
    "def create_dl_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid') \n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ===============================================================\n",
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "# ===============================================================\n",
    "# target_num_horses = [8, 9, 10, 11, 12]\n",
    "target_num_horses = [8]\n",
    "target_course_ids = [\n",
    "    102, 103, 201, 202, 301, 302, 402,\n",
    "    501, 502, 601, 602, 700, 702, 802,\n",
    "    900, 901, 1001, 1002, 1201, 1304, 1400, 1402\n",
    "]\n",
    "\n",
    "input_file = \"input_å…¨é ­_ç·¨é›†å¾Œ.csv\"\n",
    "df_all = pd.read_csv(input_file, encoding=\"utf-8\")\n",
    "df_all[\"race_date\"] = pd.to_datetime(df_all[\"race_date\"])\n",
    "print(f\"å…¨ãƒ‡ãƒ¼ã‚¿: {len(df_all)}ä»¶\")\n",
    "\n",
    "# ä¸¦ã³æ›¿ãˆ\n",
    "df_all = df_all.sort_values([\"race_id\", \"horse_number\"])\n",
    "\n",
    "# ===============================================================\n",
    "# A. éå»å®Ÿç¸¾ãƒ™ãƒ¼ã‚¹ã®è„šè³ªç®—å‡º (running_style_type_past)\n",
    "# ===============================================================\n",
    "print(\"ğŸ‡ éå»å®Ÿç¸¾ãƒ™ãƒ¼ã‚¹ã®è„šè³ªã‚’ç®—å‡ºä¸­...\")\n",
    "df_all[\"last_corner_position\"] = df_all.apply(get_last_corner_position, axis=1)\n",
    "df_all[\"last_corner_position_rate\"] = df_all[\"last_corner_position\"] / df_all[\"num_horses\"]\n",
    "\n",
    "# æ™‚ç³»åˆ—é †ã§éå»å¹³å‡ã‚’è¨ˆç®—\n",
    "df_all = df_all.sort_values([\"horse_id\", \"race_date\"])\n",
    "df_all[\"avg_last_corner_pos_rate_past\"] = (\n",
    "    df_all.groupby(\"horse_id\")[\"last_corner_position_rate\"]\n",
    "    .transform(lambda x: x.expanding().mean().shift())\n",
    ")\n",
    "df_all[\"running_style_type_past\"] = np.where(\n",
    "    df_all[\"avg_last_corner_pos_rate_past\"] < 0.5, \"front\",\n",
    "    np.where(df_all[\"avg_last_corner_pos_rate_past\"].notnull(), \"back\", np.nan)\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# å¯¾è±¡ã‚³ãƒ¼ã‚¹Ã—é ­æ•°ã«çµã‚‹ & ç‰¹å¾´é‡åŸºæœ¬ä½œæˆ\n",
    "# ===============================================================\n",
    "df_target = df_all[df_all[\"course_id\"].isin(target_course_ids)].copy()\n",
    "df_target = df_target[df_target[\"num_horses\"].isin(target_num_horses)].copy()\n",
    "\n",
    "handle_missing = True\n",
    "missing_check_features = [\"time_index_average_2\", \"jockey_place_rate_100\"]\n",
    "df_target = handle_missing_val(df_target, features=missing_check_features, enabled=handle_missing, min_horses=8)\n",
    "\n",
    "# ã‚¿ã‚¤ãƒ æŒ‡æ•°å·®ï¼ˆå±•é–‹äºˆæƒ³ã«ã‚‚ä½¿ã†ãŸã‚ã“ã“ã§ä½œæˆï¼‰\n",
    "race_avg_ti = df_target.groupby(\"race_id\")[\"time_index_average_2\"].transform(\"mean\")\n",
    "df_target[\"time_index_diff_from_avg\"] = df_target[\"time_index_average_2\"] - race_avg_ti\n",
    "\n",
    "# ===============================================================\n",
    "# B. å±•é–‹äºˆæƒ³ãƒ™ãƒ¼ã‚¹ã®è„šè³ªç®—å‡º (running_style_type_pred)\n",
    "# ===============================================================\n",
    "print(\"ğŸ‡ å±•é–‹äºˆæƒ³ãƒ™ãƒ¼ã‚¹ã®è„šè³ªã‚’ç®—å‡ºä¸­...\")\n",
    "style_map = {\"é€ƒã’\": 1, \"å…ˆè¡Œ\": 2, \"å·®ã—\": 3, \"è¿½è¾¼\": 4}\n",
    "df_target[\"style_rank_val\"] = df_target[\"style_name\"].map(style_map).fillna(3)\n",
    "\n",
    "def predict_front_back(group):\n",
    "    # 1. æˆ¦æ³•é †ã€2. æŒ‡æ•°å·®(é«˜ã„ã»ã©å‰) ã§ã‚½ãƒ¼ãƒˆ\n",
    "    group = group.sort_values([\"style_rank_val\", \"time_index_diff_from_avg\"], ascending=[True, False])\n",
    "    n = len(group)\n",
    "    mid = n // 2\n",
    "    res = pd.Series(index=group.index, dtype=str)\n",
    "    res.iloc[:mid] = \"front\"\n",
    "    res.iloc[mid:] = \"back\"\n",
    "    return res\n",
    "\n",
    "df_target[\"running_style_type_pred\"] = df_target.groupby(\"race_id\", group_keys=False).apply(predict_front_back)\n",
    "\n",
    "# ===============================================================\n",
    "# ä½¿ç”¨ã™ã‚‹è„šè³ªã‚«ãƒ©ãƒ ã®åˆ‡ã‚Šæ›¿ãˆè¨­å®š\n",
    "# ===============================================================\n",
    "# STYLE_MODE ã«åŸºã¥ã„ã¦ã€ä»¥é™ã®è¨ˆç®—ã§ä½¿ã†ãƒ¡ã‚¤ãƒ³ã‚«ãƒ©ãƒ ã‚’æ±ºå®š\n",
    "USE_STYLE_COL = f\"running_style_type_{STYLE_MODE}\"\n",
    "print(f\"ğŸ“¢ ä»Šå›ã®å‡¦ç†ã§ã¯ ã€{USE_STYLE_COL}ã€‘ ã‚’åŸºæº–ã«å‹ç‡ãƒ»å±•é–‹ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\")\n",
    "\n",
    "# ===============================================================\n",
    "# ç‰¹å¾´é‡ä½œæˆ (å‹ç‡ãƒ»ç›¸å¯¾æŒ‡æ¨™)\n",
    "# ===============================================================\n",
    "\n",
    "# (1) é¨æ‰‹å‹ç‡å¹³å‡ã¨ã®å·®\n",
    "race_avg_j = df_target.groupby(\"race_id\")[\"jockey_place_rate_100\"].transform(\"mean\")\n",
    "df_target[\"jockey_place_rate_diff_from_avg\"] = df_target[\"jockey_place_rate_100\"] - race_avg_j\n",
    "\n",
    "# (2) é¦¬ç•ªåˆ¥å‹ç‡\n",
    "df_target[\"is_win\"] = (df_target[\"finish_rank\"] == 1).astype(int)\n",
    "win_rate_by_horse_number = (\n",
    "    df_target.groupby([\"course_id\", \"horse_number\"])[\"is_win\"]\n",
    "    .mean().reset_index().rename(columns={\"is_win\": \"win_rate_by_course_horse_number\"})\n",
    ")\n",
    "df_target = df_target.merge(win_rate_by_horse_number, on=[\"course_id\", \"horse_number\"], how=\"left\")\n",
    "race_avg_h = df_target.groupby(\"race_id\")[\"win_rate_by_course_horse_number\"].transform(\"mean\")\n",
    "df_target[\"win_rate_by_course_horse_number_diff_from_avg\"] = df_target[\"win_rate_by_course_horse_number\"] - race_avg_h\n",
    "\n",
    "# (3) é¸æŠã•ã‚ŒãŸè„šè³ªã‚¿ã‚¤ãƒ—åˆ¥ã®ã‚³ãƒ¼ã‚¹å‹ç‡\n",
    "# éå»å…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è„šè³ªå‚¾å‘(overall)ã‚’ãƒãƒ¼ã‚¸ã—ã¦ç®—å‡º\n",
    "horse_mean_style_rate = df_all.groupby(\"horse_id\")[\"last_corner_position_rate\"].mean().reset_index()\n",
    "horse_mean_style_rate[\"running_style_type_overall\"] = np.where(\n",
    "    horse_mean_style_rate[\"last_corner_position_rate\"] < 0.5, \"front\", \"back\"\n",
    ")\n",
    "\n",
    "df_course_all = df_all[df_all[\"course_id\"].isin(target_course_ids)].merge(\n",
    "    horse_mean_style_rate[[\"horse_id\", \"running_style_type_overall\"]], on=\"horse_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "course_style_win_summary = (\n",
    "    df_course_all.groupby([\"course_id\", \"running_style_type_overall\"])\n",
    "    .agg(total_runs=(\"horse_id\", \"count\"), total_wins=(\"finish_rank\", lambda x: (x == 1).sum()))\n",
    "    .reset_index()\n",
    ")\n",
    "course_style_win_summary[\"win_rate_overall\"] = course_style_win_summary[\"total_wins\"] / course_style_win_summary[\"total_runs\"]\n",
    "\n",
    "# é¸æŠã—ãŸè„šè³ª(USE_STYLE_COL)ã«å¯¾ã—ã¦ãƒã‚¹ã‚¿ã‚’ãƒãƒ¼ã‚¸\n",
    "df_target = df_target.merge(\n",
    "    course_style_win_summary[[\"course_id\", \"running_style_type_overall\", \"win_rate_overall\"]],\n",
    "    left_on=[\"course_id\", USE_STYLE_COL],\n",
    "    right_on=[\"course_id\", \"running_style_type_overall\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={\"win_rate_overall\": \"running_style_win_prob\"}).drop(columns=[\"running_style_type_overall\"])\n",
    "\n",
    "race_avg_rs = df_target.groupby(\"race_id\")[\"running_style_win_prob\"].transform(\"mean\")\n",
    "df_target[\"running_style_win_prob_diff_from_avg\"] = df_target[\"running_style_win_prob\"] - race_avg_rs\n",
    "\n",
    "# (4) å±•é–‹ï¼ˆfront/backã®æ¯”ç‡ï¼‰ã«ã‚ˆã‚‹å‹ç‡\n",
    "# é¸æŠã—ãŸè„šè³ªã‚«ãƒ©ãƒ ã«åŸºã¥ã„ã¦ãƒ¬ãƒ¼ã‚¹å†…ã®æ¯”ç‡ã‚’è¨ˆç®—\n",
    "race_style_counts = df_target.groupby([\"race_id\", USE_STYLE_COL]).size().unstack(fill_value=0).reset_index()\n",
    "for col in [\"front\", \"back\"]:\n",
    "    if col not in race_style_counts.columns: race_style_counts[col] = 0\n",
    "\n",
    "race_style_counts[\"total\"] = race_style_counts[\"front\"] + race_style_counts[\"back\"]\n",
    "race_style_counts[\"ratio_front_round\"] = (race_style_counts[\"front\"] / race_style_counts[\"total\"]).round(2)\n",
    "race_style_counts[\"ratio_back_round\"] = (race_style_counts[\"back\"] / race_style_counts[\"total\"]).round(2)\n",
    "\n",
    "df_target = df_target.merge(\n",
    "    race_style_counts[[\"race_id\", \"ratio_front_round\", \"ratio_back_round\"]], on=\"race_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# å±•é–‹åˆ¥å‹ç‡ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿å†…ã§è¨ˆç®—ï¼‰\n",
    "style_win_ratio_df = df_target.groupby([\n",
    "    \"course_id\", USE_STYLE_COL, \"ratio_front_round\", \"ratio_back_round\"\n",
    "]).agg(total_horses=(\"race_id\", \"count\"), total_wins=(\"is_win\", \"sum\")).reset_index()\n",
    "style_win_ratio_df[\"win_rate\"] = style_win_ratio_df[\"total_wins\"] / style_win_ratio_df[\"total_horses\"]\n",
    "\n",
    "df_target = df_target.merge(\n",
    "    style_win_ratio_df[[\"course_id\", USE_STYLE_COL, \"ratio_front_round\", \"ratio_back_round\", \"win_rate\"]],\n",
    "    on=[\"course_id\", USE_STYLE_COL, \"ratio_front_round\", \"ratio_back_round\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={\"win_rate\": \"style_win_prob_by_ratio\"})\n",
    "\n",
    "race_avg_ratio = df_target.groupby(\"race_id\")[\"style_win_prob_by_ratio\"].transform(\"mean\")\n",
    "df_target[\"style_win_prob_by_ratio_diff_from_avg\"] = df_target[\"style_win_prob_by_ratio\"] - race_avg_ratio\n",
    "\n",
    "df_target.to_csv('C:\\\\Users\\\\ryo\\\\Downloads\\\\df_target.csv', index=False)\n",
    "\n",
    "print(\"âœ… ç‰¹å¾´é‡ä½œæˆå®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbfceaa5-37e1-4829-8f58-7a77f6a45c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š å±•é–‹äºˆæƒ³ã®æ­£ç­”ç‡ï¼ˆç­”ãˆåˆã‚ã›ï¼‰ã‚’ç®—å‡ºä¸­...\n",
      "------------------------------\n",
      "âœ… å…¨ä½“æ­£ç­”ç‡: 68.73%\n",
      "------------------------------\n",
      "â–  è„šè³ª(style_name)åˆ¥ã®äºˆæƒ³æ­£ç­”ç‡:\n",
      "style_name\n",
      "é€ƒã’    80.10%\n",
      "å…ˆè¡Œ    76.13%\n",
      "è¿½è¾¼    76.08%\n",
      "å·®ã—    63.61%\n",
      "Name: is_correct, dtype: object\n",
      "------------------------------\n",
      "â–  ã‚³ãƒ¼ã‚¹åˆ¥æ­£ç­”ç‡ï¼ˆãƒ¯ãƒ¼ã‚¹ãƒˆ5ï¼‰: â€»æ”¹å–„ã®ãƒ’ãƒ³ãƒˆã«ãªã‚Šã¾ã™\n",
      "course_id\n",
      "702     63.02%\n",
      "1201    63.75%\n",
      "901     64.73%\n",
      "700     65.33%\n",
      "802     65.96%\n",
      "Name: is_correct, dtype: object\n",
      "------------------------------\n",
      "â–  äºˆæ¸¬ã®å‚¾å‘ï¼ˆè¡Œï¼šå®Ÿéš› / åˆ—ï¼šäºˆæ¸¬ï¼‰\n",
      "running_style_type_pred    back   front\n",
      "actual_style                           \n",
      "back                     69.38%  30.62%\n",
      "front                    31.93%  68.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryo\\AppData\\Local\\Temp\\ipykernel_7004\\265104599.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  eval_df[\"actual_style\"] = eval_df.groupby(\"race_id\", group_keys=False).apply(label_actual_front_back)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# å±•é–‹äºˆæƒ³ (running_style_type_pred) ã®ç­”ãˆåˆã‚ã›\n",
    "# ===============================================================\n",
    "print(\"\\nğŸ“Š å±•é–‹äºˆæƒ³ã®æ­£ç­”ç‡ï¼ˆç­”ãˆåˆã‚ã›ï¼‰ã‚’ç®—å‡ºä¸­...\")\n",
    "\n",
    "def evaluate_style_prediction(df):\n",
    "    # 1. æœ€çµ‚ã‚³ãƒ¼ãƒŠãƒ¼é †ä½ã®å–å¾— (æ—¢å­˜ã® get_last_corner_position ã‚’æ´»ç”¨)\n",
    "    # â€» get_last_corner_position ã¯ position_4, 3, 2, 1 ã®é †ã«æœ€åˆã«è¦‹ã¤ã‹ã£ãŸéæ¬ æå€¤ã‚’è¿”ã™\n",
    "    df[\"actual_last_corner_pos\"] = df.apply(get_last_corner_position, axis=1)\n",
    "    \n",
    "    # æœ€çµ‚ã‚³ãƒ¼ãƒŠãƒ¼é †ä½ãŒå–å¾—ã§ããªã„ãƒ‡ãƒ¼ã‚¿ã¯è©•ä¾¡ã‹ã‚‰é™¤å¤–\n",
    "    eval_df = df[df[\"actual_last_corner_pos\"].notnull()].copy()\n",
    "    \n",
    "    def label_actual_front_back(group):\n",
    "        # ãƒ¬ãƒ¼ã‚¹å†…ã§ã®æœ€çµ‚ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½ã«åŸºã¥ã„ã¦ã€å®Ÿéš›ã®çµæœ(actual_style)ã‚’åˆ¤å®š\n",
    "        # ã“ã“ã§ã‚‚äºˆæ¸¬æ™‚ã¨åŒã˜ãã€Œæ˜‡é †ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½åŠåˆ†ã€ã‚’ front ã¨ã™ã‚‹\n",
    "        group = group.sort_values(\"actual_last_corner_pos\", ascending=True)\n",
    "        n = len(group)\n",
    "        mid = n // 2\n",
    "        \n",
    "        res = pd.Series(index=group.index, dtype=str)\n",
    "        res.iloc[:mid] = \"front\"\n",
    "        res.iloc[mid:] = \"back\"\n",
    "        return res\n",
    "\n",
    "    # å®Ÿéš›ã®å±•é–‹çµæœã‚’ãƒ©ãƒ™ãƒ«ä»˜ã‘\n",
    "    eval_df[\"actual_style\"] = eval_df.groupby(\"race_id\", group_keys=False).apply(label_actual_front_back)\n",
    "    \n",
    "    # 2. äºˆæ¸¬ã¨å®Ÿç¸¾ã®æ¯”è¼ƒ\n",
    "    # äºˆæ¸¬(running_style_type_pred) ã¨ å®Ÿç¸¾(actual_style) ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹\n",
    "    eval_df[\"is_correct\"] = (eval_df[\"running_style_type_pred\"] == eval_df[\"actual_style\"])\n",
    "    \n",
    "    # 3. å…¨ä½“æ­£ç­”ç‡\n",
    "    overall_accuracy = eval_df[\"is_correct\"].mean()\n",
    "    \n",
    "    # 4. è„šè³ª(style_name)ã”ã¨ã®æ­£ç­”ç‡\n",
    "    style_accuracy = eval_df.groupby(\"style_name\")[\"is_correct\"].mean().sort_values(ascending=False)\n",
    "    \n",
    "    # 5. ã‚³ãƒ¼ã‚¹ã”ã¨ã®æ­£ç­”ç‡ï¼ˆä¸Šä½5ãƒ»ä¸‹ä½5ï¼‰\n",
    "    course_accuracy = eval_df.groupby(\"course_id\")[\"is_correct\"].mean()\n",
    "    \n",
    "    return overall_accuracy, style_accuracy, course_accuracy, eval_df\n",
    "\n",
    "# å®Ÿè¡Œ\n",
    "accuracy, style_acc, course_acc, result_df = evaluate_style_prediction(df_target)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"âœ… å…¨ä½“æ­£ç­”ç‡: {accuracy:.2%}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"â–  è„šè³ª(style_name)åˆ¥ã®äºˆæƒ³æ­£ç­”ç‡:\")\n",
    "print(style_acc.map(lambda x: f\"{x:.2%}\"))\n",
    "print(\"-\" * 30)\n",
    "print(\"â–  ã‚³ãƒ¼ã‚¹åˆ¥æ­£ç­”ç‡ï¼ˆãƒ¯ãƒ¼ã‚¹ãƒˆ5ï¼‰: â€»æ”¹å–„ã®ãƒ’ãƒ³ãƒˆã«ãªã‚Šã¾ã™\")\n",
    "print(course_acc.sort_values().head(5).map(lambda x: f\"{x:.2%}\"))\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# æ··åŒè¡Œåˆ—ï¼ˆãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼‰ã®è¡¨ç¤º\n",
    "confusion_matrix = pd.crosstab(result_df['actual_style'], result_df['running_style_type_pred'], normalize='index')\n",
    "print(\"â–  äºˆæ¸¬ã®å‚¾å‘ï¼ˆè¡Œï¼šå®Ÿéš› / åˆ—ï¼šäºˆæ¸¬ï¼‰\")\n",
    "print(confusion_matrix.map(lambda x: f\"{x:.2%}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a569e8d9-e05a-49bd-b2a6-491b19906acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryo\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš© ãƒ¢ãƒ¼ãƒ‰: ä¸€æ‹¬(Hold-out) / ã‚°ãƒ«ãƒ¼ãƒ—: (8,)\n",
      "âœ… å…¨å·¥ç¨‹å®Œäº†ã€‚df_targetã«äºˆæ¸¬å€¤ãŒæ ¼ç´ã•ã‚Œã¾ã—ãŸã€‚\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼šäººæ°—åˆ¥ã®çš„ä¸­ç‡ãƒ»å›åç‡ (å˜å‹æƒ³å®š)\n",
      "======================================================================\n",
      "\n",
      "â–¼ äººæ°—åˆ¥æˆç¸¾ä¸€è¦§ (1ç‚¹ 100å††æƒ³å®š)\n",
      "\n",
      "--- Model: XGBoost ---\n",
      "            ãƒ¬ãƒ¼ã‚¹æ•°  çš„ä¸­æ•°  çš„ä¸­ç‡(%)  å›åç‡(%)\n",
      "popularity                           \n",
      "1.0          569  305   53.60   83.36\n",
      "2.0          265   78   29.43   91.77\n",
      "3.0          141   21   14.89   69.08\n",
      "4.0           55    7   12.73   92.91\n",
      "5.0           22    5   22.73  437.73\n",
      "6.0            8    0    0.00    0.00\n",
      "7.0            3    0    0.00    0.00\n",
      "\n",
      "--- Model: DeepLearning ---\n",
      "            ãƒ¬ãƒ¼ã‚¹æ•°  çš„ä¸­æ•°  çš„ä¸­ç‡(%)  å›åç‡(%)\n",
      "popularity                           \n",
      "1.0          632  327   51.74   78.29\n",
      "2.0          245   67   27.35   85.02\n",
      "3.0          118   17   14.41   70.76\n",
      "4.0           42    7   16.67  112.62\n",
      "5.0           14    4   28.57  300.71\n",
      "6.0            9    0    0.00    0.00\n",
      "7.0            2    0    0.00    0.00\n",
      "\n",
      "========================================\n",
      "ğŸ† ãƒ¢ãƒ‡ãƒ«å…¨ä½“æˆç¸¾ï¼ˆå…¨äººæ°—åˆè¨ˆï¼‰\n",
      "========================================\n",
      "              total_races  total_hits  çš„ä¸­ç‡(%)  å›åç‡(%)\n",
      "model                                                \n",
      "DeepLearning         1062         422   39.74   82.49\n",
      "XGBoost              1063         416   39.13   90.53\n"
     ]
    }
   ],
   "source": [
    "USE_WALK_FORWARD = False\n",
    "\n",
    "# ===============================================================\n",
    "# ã‚«ãƒ†ã‚´ãƒªæ•°å€¤åŒ–ã¨One-Hot\n",
    "# ===============================================================\n",
    "df_target[\"relative_rank\"] = df_target[\"finish_rank\"] / df_target[\"num_horses\"]\n",
    "df_target_encoded = df_target.copy()\n",
    "df_target_encoded['course_id_onehot'] = df_target_encoded['course_id']\n",
    "df_target_encoded = pd.get_dummies(df_target_encoded, columns=['course_id_onehot'], prefix=\"course\")\n",
    "\n",
    "# ===============================================================\n",
    "# Walk-Forward / ä¸€æ‹¬å­¦ç¿’ å…±é€šè¨­å®š\n",
    "# ===============================================================\n",
    "features = [\"time_index_diff_from_avg\", \"jockey_place_rate_diff_from_avg\", \"win_rate_by_course_horse_number_diff_from_avg\", \n",
    "            \"running_style_win_prob_diff_from_avg\", \"style_win_prob_by_ratio_diff_from_avg\"]\n",
    "\n",
    "start_year, end_year = 2021, 2024\n",
    "num_train_races = 1600\n",
    "bet_amount = 100\n",
    "scaler = StandardScaler()\n",
    "\n",
    "grouping_keys = []\n",
    "if GROUP_BY_COURSE: grouping_keys.append(\"course_id\")\n",
    "if GROUP_BY_NUM_HORSES: grouping_keys.append(\"num_horses\")\n",
    "\n",
    "group_iter = df_target_encoded.groupby(grouping_keys) if grouping_keys else [((), df_target_encoded)]\n",
    "all_wf_predictions_xgb, all_wf_predictions_dl = [], []\n",
    "\n",
    "# ===============================================================\n",
    "# ğŸ” å­¦ç¿’ãƒ»äºˆæ¸¬ãƒ«ãƒ¼ãƒ—ï¼ˆWFåˆ‡ã‚Šæ›¿ãˆã‚¹ã‚¤ãƒƒãƒå®Ÿè£…ï¼‰\n",
    "# ===============================================================\n",
    "for group_key, df_course_original in group_iter:\n",
    "    group_title = str(group_key)\n",
    "    df_course = df_course_original[(df_course_original[\"race_date\"].dt.year >= start_year) & (df_course_original[\"race_date\"].dt.year <= end_year)].copy()\n",
    "    if len(df_course[\"race_id\"].unique()) <= num_train_races: continue\n",
    "    \n",
    "    df_course = df_course.sort_values([\"race_date\", \"race_id\", \"horse_number\"]).reset_index(drop=True)\n",
    "    race_ids = df_course[\"race_id\"].unique()\n",
    "    current_features = features.copy()\n",
    "    if GROUP_BY_COURSE and f'course_{group_key[0]}' in df_course.columns:\n",
    "        current_features.append(f'course_{group_key[0]}')\n",
    "    \n",
    "    dl_model = create_dl_model(len(current_features))\n",
    "    wf_predictions_xgb_course, wf_predictions_dl_course = [], []\n",
    "\n",
    "    print(f\"\\nğŸš© ãƒ¢ãƒ¼ãƒ‰: {'Walk-Forward' if USE_WALK_FORWARD else 'ä¸€æ‹¬(Hold-out)'} / ã‚°ãƒ«ãƒ¼ãƒ—: {group_title}\")\n",
    "\n",
    "    if not USE_WALK_FORWARD:\n",
    "        # --- ã€é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ã€‘ä¸€æ‹¬å­¦ç¿’ ---\n",
    "        train_rids = race_ids[:num_train_races]\n",
    "        test_rids = race_ids[num_train_races:]\n",
    "        \n",
    "        df_train = df_course[df_course[\"race_id\"].isin(train_rids) & (df_course[\"popularity\"] != 1)]\n",
    "        df_test = df_course[df_course[\"race_id\"].isin(test_rids)].copy()\n",
    "        \n",
    "        X_train, y_train = df_train[current_features], (df_train[\"relative_rank\"] <= 0.5).astype(int)\n",
    "        X_test = df_test[current_features]\n",
    "        \n",
    "        # XGBoost\n",
    "        model_xgb = XGBClassifier(objective=\"binary:logistic\", n_estimators=100, learning_rate=0.06, max_depth=3, random_state=42)\n",
    "        model_xgb.fit(X_train, y_train)\n",
    "        df_test[\"pred_prob_xgb\"] = model_xgb.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # DL\n",
    "        X_train_scaled, X_test_scaled = scaler.fit_transform(X_train), scaler.transform(X_test)\n",
    "        dl_model.fit(X_train_scaled, y_train, epochs=20, batch_size=16, verbose=0)\n",
    "        df_test[\"pred_prob_dl\"] = dl_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "        \n",
    "        # çµæœæ ¼ç´ï¼ˆå¾Œç¶šã®é›†è¨ˆç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«åˆã‚ã›ã‚‹ï¼‰\n",
    "        for rid in test_rids:\n",
    "            df_val = df_test[df_test[\"race_id\"] == rid].copy()\n",
    "            num_select = int(df_val[\"num_horses\"].iloc[0] / 2)\n",
    "            # XGB\n",
    "            df_v_xgb = df_val.sort_values(\"pred_prob_xgb\", ascending=False).assign(selected=0, model=\"XGBoost\")\n",
    "            df_v_xgb.iloc[:num_select, df_v_xgb.columns.get_loc(\"selected\")] = 1\n",
    "            wf_predictions_xgb_course.append(df_v_xgb)\n",
    "            # DL\n",
    "            df_v_dl = df_val.sort_values(\"pred_prob_dl\", ascending=False).assign(selected=0, model=\"DeepLearning\")\n",
    "            df_v_dl.iloc[:num_select, df_v_dl.columns.get_loc(\"selected\")] = 1\n",
    "            wf_predictions_dl_course.append(df_v_dl)\n",
    "            \n",
    "    else:\n",
    "        # --- ã€é‡åšãƒ¢ãƒ¼ãƒ‰ã€‘Walk-Forward ---\n",
    "        for i in tqdm(range(num_train_races, len(race_ids)), desc=f\"WF {group_title}\"):\n",
    "            train_races, val_race = race_ids[:i], race_ids[i]\n",
    "            df_train = df_course[df_course[\"race_id\"].isin(train_races) & (df_course[\"popularity\"] != 1)]\n",
    "            df_val = df_course[df_course[\"race_id\"] == val_race].copy()\n",
    "            X_train, y_train = df_train[current_features], (df_train[\"relative_rank\"] <= 0.5).astype(int)\n",
    "            X_val = df_val[current_features]\n",
    "            \n",
    "            # XGB\n",
    "            model_xgb = XGBClassifier(objective=\"binary:logistic\", n_estimators=100, learning_rate=0.06, max_depth=3, random_state=42)\n",
    "            model_xgb.fit(X_train, y_train)\n",
    "            df_val[\"pred_prob_xgb\"] = model_xgb.predict_proba(X_val)[:, 1]\n",
    "            # DL (WFæ™‚ã¯Epochã‚’æ¸›ã‚‰ã™ã¨æ™‚çŸ­)\n",
    "            X_tr_s, X_va_s = scaler.fit_transform(X_train), scaler.transform(X_val)\n",
    "            dl_model.fit(X_tr_s, y_train, epochs=2, batch_size=16, verbose=0)\n",
    "            df_val[\"pred_prob_dl\"] = dl_model.predict(X_va_s, verbose=0).flatten()\n",
    "            \n",
    "            num_select = int(df_val[\"num_horses\"].iloc[0] / 2)\n",
    "            wf_predictions_xgb_course.append(df_val.sort_values(\"pred_prob_xgb\", ascending=False).assign(selected=0, model=\"XGBoost\"))\n",
    "            wf_predictions_dl_course.append(df_val.sort_values(\"pred_prob_dl\", ascending=False).assign(selected=0, model=\"DeepLearning\"))\n",
    "            # é¸æŠãƒ•ãƒ©ã‚°è¨­å®š\n",
    "            wf_predictions_xgb_course[-1].iloc[:num_select, -2] = 1\n",
    "            wf_predictions_dl_course[-1].iloc[:num_select, -2] = 1\n",
    "\n",
    "    all_wf_predictions_xgb.append(pd.concat(wf_predictions_xgb_course))\n",
    "    all_wf_predictions_dl.append(pd.concat(wf_predictions_dl_course))\n",
    "\n",
    "# ===============================================================\n",
    "# æœ€çµ‚ãƒãƒ¼ã‚¸ã¨å®Œäº†\n",
    "# ===============================================================\n",
    "wf_all_xgb = pd.concat(all_wf_predictions_xgb)\n",
    "wf_all_dl = pd.concat(all_wf_predictions_dl)\n",
    "df_target = df_target.merge(wf_all_xgb[[\"race_id\", \"horse_number\", \"pred_prob_xgb\"]], on=[\"race_id\", \"horse_number\"], how=\"left\")\n",
    "df_target = df_target.merge(wf_all_dl[[\"race_id\", \"horse_number\", \"pred_prob_dl\"]], on=[\"race_id\", \"horse_number\"], how=\"left\")\n",
    "\n",
    "print(\"âœ… å…¨å·¥ç¨‹å®Œäº†ã€‚df_targetã«äºˆæ¸¬å€¤ãŒæ ¼ç´ã•ã‚Œã¾ã—ãŸã€‚\")\n",
    "\n",
    "# ===============================================================\n",
    "# ğŸ“ˆ çš„ä¸­ç‡ãƒ»å›åç‡ã®é›†è¨ˆã¨å‡ºåŠ›ï¼ˆäººæ°—åˆ¥ï¼‰\n",
    "# ===============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼šäººæ°—åˆ¥ã®çš„ä¸­ç‡ãƒ»å›åç‡ (å˜å‹æƒ³å®š)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. äºˆæ¸¬çµæœã®æ•´å½¢ï¼ˆXGBoostã¨DLã‚’çµ±åˆï¼‰\n",
    "# å„ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«äºˆæ¸¬ç¢ºç‡(pred_prob)ãŒæœ€å¤§ã®é¦¬ã‚’ã€Œæœ¬å‘½ã€ã¨ã—ã¦æŠ½å‡º\n",
    "df_xgb_top = wf_all_xgb.loc[wf_all_xgb.groupby(\"race_id\")[\"pred_prob_xgb\"].idxmax()].copy()\n",
    "df_dl_top = wf_all_dl.loc[wf_all_dl.groupby(\"race_id\")[\"pred_prob_dl\"].idxmax()].copy()\n",
    "\n",
    "df_xgb_top[\"model\"] = \"XGBoost\"\n",
    "df_dl_top[\"model\"] = \"DeepLearning\"\n",
    "\n",
    "# çš„ä¸­åˆ¤å®šï¼ˆ1ç€ã‹ã©ã†ã‹ï¼‰ã¨æ‰•æˆ»é‡‘ã®è¨ˆç®—\n",
    "def calc_stats(df):\n",
    "    df[\"is_hit\"] = (df[\"finish_rank\"] == 1).astype(int)\n",
    "    df[\"payout\"] = df[\"is_hit\"] * df[\"odds\"] * bet_amount\n",
    "    return df\n",
    "\n",
    "df_xgb_top = calc_stats(df_xgb_top)\n",
    "df_dl_top = calc_stats(df_dl_top)\n",
    "\n",
    "# ä¸¡ãƒ¢ãƒ‡ãƒ«ã‚’çµåˆã—ã¦ä¸€æ‹¬é›†è¨ˆ\n",
    "df_eval_all = pd.concat([df_xgb_top, df_dl_top], axis=0)\n",
    "\n",
    "# 2. äººæ°—åˆ¥ã®é›†è¨ˆå‡¦ç†\n",
    "summary_list = []\n",
    "models = [\"XGBoost\", \"DeepLearning\"]\n",
    "\n",
    "for m in models:\n",
    "    df_m = df_eval_all[df_eval_all[\"model\"] == m]\n",
    "    \n",
    "    # äººæ°—ã”ã¨ã«é›†è¨ˆ\n",
    "    stats_pop = df_m.groupby(\"popularity\").agg(\n",
    "        total_races=(\"race_id\", \"count\"),\n",
    "        hits=(\"is_hit\", \"sum\"),\n",
    "        total_payout=(\"payout\", \"sum\")\n",
    "    ).reset_index()\n",
    "    \n",
    "    # æŒ‡æ¨™ã®è¨ˆç®—\n",
    "    stats_pop[\"accuracy\"] = (stats_pop[\"hits\"] / stats_pop[\"total_races\"] * 100).round(2)\n",
    "    stats_pop[\"roi\"] = (stats_pop[\"total_payout\"] / (stats_pop[\"total_races\"] * bet_amount) * 100).round(2)\n",
    "    stats_pop[\"model\"] = m\n",
    "    summary_list.append(stats_pop)\n",
    "\n",
    "# 3. çµæœã®è¡¨ç¤º\n",
    "final_summary = pd.concat(summary_list, axis=0)\n",
    "\n",
    "# è¦‹ã‚„ã™ãã™ã‚‹ãŸã‚ã«ãƒ”ãƒœãƒƒãƒˆï¼ˆãƒ¢ãƒ‡ãƒ«ã‚’æ¨ªã«ä¸¦ã¹ã‚‹ï¼‰\n",
    "print(f\"\\nâ–¼ äººæ°—åˆ¥æˆç¸¾ä¸€è¦§ (1ç‚¹ 100å††æƒ³å®š)\")\n",
    "for m in models:\n",
    "    print(f\"\\n--- Model: {m} ---\")\n",
    "    subset = final_summary[final_summary[\"model\"] == m].copy()\n",
    "    # ä¸Šä½äººæ°—ï¼ˆ1ã€œ10äººæ°—ç¨‹åº¦ï¼‰ã«çµã£ã¦è¡¨ç¤º\n",
    "    subset = subset[subset[\"popularity\"] <= 10].set_index(\"popularity\")\n",
    "    \n",
    "    # åˆ—åã®æ•´ç†\n",
    "    subset.columns = [\"ãƒ¬ãƒ¼ã‚¹æ•°\", \"çš„ä¸­æ•°\", \"ç·æ‰•æˆ»\", \"çš„ä¸­ç‡(%)\", \"å›åç‡(%)\", \"ãƒ¢ãƒ‡ãƒ«å\"]\n",
    "    print(subset[[\"ãƒ¬ãƒ¼ã‚¹æ•°\", \"çš„ä¸­æ•°\", \"çš„ä¸­ç‡(%)\", \"å›åç‡(%)\"]])\n",
    "\n",
    "# 4. å…¨ä½“æˆç¸¾ã®è¡¨ç¤º\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"ğŸ† ãƒ¢ãƒ‡ãƒ«å…¨ä½“æˆç¸¾ï¼ˆå…¨äººæ°—åˆè¨ˆï¼‰\")\n",
    "print(\"=\"*40)\n",
    "total_summary = final_summary.groupby(\"model\").agg(\n",
    "    total_races=(\"total_races\", \"sum\"),\n",
    "    total_hits=(\"hits\", \"sum\"),\n",
    "    total_invest=(\"total_races\", lambda x: (x * bet_amount).sum()),\n",
    "    total_payout=(\"total_payout\", \"sum\")\n",
    ")\n",
    "total_summary[\"çš„ä¸­ç‡(%)\"] = (total_summary[\"total_hits\"] / total_summary[\"total_races\"] * 100).round(2)\n",
    "total_summary[\"å›åç‡(%)\"] = (total_summary[\"total_payout\"] / total_summary[\"total_invest\"] * 100).round(2)\n",
    "print(total_summary[[\"total_races\", \"total_hits\", \"çš„ä¸­ç‡(%)\", \"å›åç‡(%)\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09986a77-f928-4a3a-8ab3-6ad1c1873fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸš€ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ [ã‚½ãƒ¼ã‚¹: dl / åˆæ„ãƒ•ã‚£ãƒ«ã‚¿: False]\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monte Carlo Simulating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1084/1084 [00:42<00:00, 25.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… å›åç‡: 94.06%\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# ğŸ‡ è¨­å®šå¤‰æ•°ï¼ˆã“ã“ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ï¼‰\n",
    "# ===============================================================\n",
    "# 1. ãƒ™ãƒ¼ã‚¹ç¢ºç‡ã®é¸æŠ: \"xgb\", \"dl\", \"ensemble\" (ä¸¡æ–¹ã®å¹³å‡)\n",
    "PROB_SOURCE = \"dl\"\n",
    "\n",
    "# 2. ä¸¡ãƒ¢ãƒ‡ãƒ«ã®åˆæ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: Trueãªã‚‰XGBã¨DLã®ä¸¡æ–¹ã§æœŸå¾…å€¤ > 1.0ãŒå¿…è¦\n",
    "USE_STRICT_AGREEMENT = False\n",
    "\n",
    "BET_STRATEGY = \"dutching\"  # \"dutching\" or \"edge\"\n",
    "AVOID_NEGATIVE_RETURN = True\n",
    "RE_CALC_BY_EXCLUDING = True\n",
    "\n",
    "# äººæ°—åˆ¥ãƒ»æœŸå¾…å€¤ãƒ•ã‚£ãƒ«ã‚¿é–¾å€¤ï¼ˆåˆæ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é€šéå¾Œã®æœ€çµ‚åˆ¤å®šã«ä½¿ç”¨ï¼‰\n",
    "EV_THRESHOLD_MAP = {\n",
    "    1: 1.20,\n",
    "    2: 1.20,\n",
    "    3: 1.20,\n",
    "    4: 1.20,\n",
    "    \"default\": 1.00 \n",
    "}\n",
    "\n",
    "# ===============================================================\n",
    "# ğŸ‡ ã€æœ€çµ‚å·¥ç¨‹ã€‘ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ãƒ»ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "# ===============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ğŸš€ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ [ã‚½ãƒ¼ã‚¹: {PROB_SOURCE} / åˆæ„ãƒ•ã‚£ãƒ«ã‚¿: {USE_STRICT_AGREEMENT}]\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "target_races = wf_all_xgb[\"race_id\"].unique()\n",
    "mc_bet_results = []\n",
    "all_horses_log = [] \n",
    "\n",
    "for rid in tqdm(target_races, desc=\"Monte Carlo Simulating\"):\n",
    "    race_df = wf_all_xgb[wf_all_xgb[\"race_id\"] == rid].copy()\n",
    "    if len(race_df) == 0: continue\n",
    "    \n",
    "    # --- A. å„ªå‹ç‡ãƒ»æœŸå¾…å€¤ã®ç®—å‡º (XGBoost) ---\n",
    "    win_counts_xgb = {h: 0 for h in race_df[\"horse_number\"]}\n",
    "    probs_xgb = race_df[\"pred_prob_xgb\"].values\n",
    "    h_nums = race_df[\"horse_number\"].values\n",
    "    for _ in range(num_simulations):\n",
    "        idx = np.argmax(probs_xgb + np.random.normal(0, 0.1, len(probs_xgb)))\n",
    "        win_counts_xgb[h_nums[idx]] += 1\n",
    "    race_df[\"ev_xgb\"] = (race_df[\"horse_number\"].map(win_counts_xgb) / num_simulations) * race_df[\"odds\"]\n",
    "\n",
    "    # --- B. å„ªå‹ç‡ãƒ»æœŸå¾…å€¤ã®ç®—å‡º (Deep Learning) ---\n",
    "    win_counts_dl = {h: 0 for h in race_df[\"horse_number\"]}\n",
    "    probs_dl = race_df[\"pred_prob_dl\"].values\n",
    "    for _ in range(num_simulations):\n",
    "        idx = np.argmax(probs_dl + np.random.normal(0, 0.1, len(probs_dl)))\n",
    "        win_counts_dl[h_nums[idx]] += 1\n",
    "    race_df[\"ev_dl\"] = (race_df[\"horse_number\"].map(win_counts_dl) / num_simulations) * race_df[\"odds\"]\n",
    "\n",
    "    # --- C. ãƒ¡ã‚¤ãƒ³ã§ä½¿ç”¨ã™ã‚‹æœŸå¾…å€¤(expected_value)ã¨å„ªå‹ç‡ã®æ±ºå®š ---\n",
    "    if PROB_SOURCE == \"xgb\":\n",
    "        race_df[\"expected_value\"] = race_df[\"ev_xgb\"]\n",
    "        race_df[\"win_prob_sim\"] = race_df[\"horse_number\"].map(win_counts_xgb) / num_simulations\n",
    "    elif PROB_SOURCE == \"dl\":\n",
    "        race_df[\"expected_value\"] = race_df[\"ev_dl\"]\n",
    "        race_df[\"win_prob_sim\"] = race_df[\"horse_number\"].map(win_counts_dl) / num_simulations\n",
    "    else: # ensemble\n",
    "        race_df[\"expected_value\"] = (race_df[\"ev_xgb\"] + race_df[\"ev_dl\"]) / 2\n",
    "        race_df[\"win_prob_sim\"] = ((race_df[\"horse_number\"].map(win_counts_xgb) + race_df[\"horse_number\"].map(win_counts_dl)) / 2) / num_simulations\n",
    "\n",
    "    # --- D. è³¼å…¥å€™è£œé¸å®š ---\n",
    "    # 1. äººæ°—åˆ¥é–¾å€¤ã®é©ç”¨\n",
    "    race_df[\"ev_threshold\"] = race_df[\"popularity\"].apply(lambda p: EV_THRESHOLD_MAP.get(p, EV_THRESHOLD_MAP[\"default\"]))\n",
    "    \n",
    "    # 2. æ¡ä»¶åˆ¤å®š\n",
    "    mask = (race_df[\"expected_value\"] > race_df[\"ev_threshold\"])\n",
    "    if USE_STRICT_AGREEMENT:\n",
    "        # ä¸¡æ–¹ã®ãƒ¢ãƒ‡ãƒ«ã§æœŸå¾…å€¤ãŒ1.0ã‚’è¶…ãˆã¦ã„ã‚‹ã¨ã„ã†ã€Œå®ˆã‚Šã€ã®æ¡ä»¶ã‚’è¿½åŠ \n",
    "        mask = mask & (race_df[\"ev_xgb\"] > 1.0) & (race_df[\"ev_dl\"] > 1.0)\n",
    "    \n",
    "    ev_candidates = race_df[mask].sort_values(\"win_prob_sim\", ascending=False).head(3).copy()\n",
    "    \n",
    "    # --- E. åˆæˆã‚ªãƒƒã‚ºé™¤å¤–ãƒ»è³‡é‡‘é…åˆ† (ä»¥å‰ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶™æ‰¿) ---\n",
    "    if not ev_candidates.empty and AVOID_NEGATIVE_RETURN:\n",
    "        while not ev_candidates.empty:\n",
    "            total_inv_odds = (1.0 / ev_candidates[\"odds\"]).sum()\n",
    "            if (1.0 / total_inv_odds) >= 1.0: break \n",
    "            if RE_CALC_BY_EXCLUDING:\n",
    "                ev_candidates = ev_candidates.drop(ev_candidates[\"odds\"].idxmin())\n",
    "            else:\n",
    "                ev_candidates = ev_candidates.iloc[0:0]; break\n",
    "\n",
    "    race_df[\"is_purchased\"] = False\n",
    "    race_df[\"bet_amount\"] = 0.0\n",
    "    if not ev_candidates.empty:\n",
    "        inv_odds = 1.0 / ev_candidates[\"odds\"]\n",
    "        # æˆ¦ç•¥ã«å¿œã˜ãŸåˆ†é…\n",
    "        if BET_STRATEGY == \"dutching\":\n",
    "            amounts = (inv_odds / inv_odds.sum()) * bet_total_limit\n",
    "        else: # edge\n",
    "            edge = ev_candidates[\"expected_value\"] - 1.0\n",
    "            amounts = (edge / edge.sum()) * bet_total_limit\n",
    "            \n",
    "        for i, (idx, row) in enumerate(ev_candidates.iterrows()):\n",
    "            race_df.loc[race_df[\"horse_number\"] == row[\"horse_number\"], [\"is_purchased\", \"bet_amount\"]] = [True, amounts.iloc[i]]\n",
    "\n",
    "    race_df[\"payout\"] = (race_df[\"finish_rank\"] == 1).astype(int) * race_df[\"odds\"] * race_df[\"bet_amount\"]\n",
    "    all_horses_log.append(race_df)\n",
    "    if race_df[\"is_purchased\"].any():\n",
    "        mc_bet_results.append(race_df[race_df[\"is_purchased\"]])\n",
    "\n",
    "# é›†è¨ˆãƒ»å‡ºåŠ›\n",
    "df_full_log = pd.concat(all_horses_log)\n",
    "df_full_log.to_csv(\"C:\\\\Users\\\\ryo\\\\Downloads\\\\simulation_detail_log.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "if mc_bet_results:\n",
    "    df_res = pd.concat(mc_bet_results)\n",
    "    print(f\"\\nâœ… å›åç‡: {df_res['payout'].sum() / df_res['bet_amount'].sum() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "935dca4d-045c-430f-866b-e3f980df9121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ æˆ¦ç•¥è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ [è¨­å®š: dutching / é™¤å¤–è¨­å®š: True]\n",
      "================================================================================\n",
      "\n",
      "â–¼ ã€æˆ¦ç•¥â‘ ï¼šæ—¢å­˜ã€‘äºˆæ¸¬1ä½ 1ç‚¹è²·ã„ äººæ°—åˆ¥æˆç¸¾\n",
      " äººæ°—  è³¼å…¥ãƒ¬ãƒ¼ã‚¹æ•°  çš„ä¸­æ•°    æ‰•æˆ»åˆè¨ˆ  çš„ä¸­ç‡(%)  å›åç‡(%)\n",
      "1.0     569  305 47430.0   53.60   83.36\n",
      "2.0     265   78 24320.0   29.43   91.77\n",
      "3.0     141   21  9740.0   14.89   69.08\n",
      "4.0      55    7  5110.0   12.73   92.91\n",
      "5.0      22    5  9630.0   22.73  437.73\n",
      "6.0       8    0     0.0    0.00    0.00\n",
      "7.0       3    0     0.0    0.00    0.00\n",
      "\n",
      "â–¼ ã€æˆ¦ç•¥â‘¡ï¼šæ–°æˆ¦ç•¥ã€‘MCæœŸå¾…å€¤ å¤šç‚¹è²·ã„ äººæ°—åˆ¥æˆç¸¾\n",
      " äººæ°—  è³¼å…¥ç¥¨æ•°         æŠ•è³‡åˆè¨ˆ  çš„ä¸­æ•°         æ‰•æˆ»åˆè¨ˆ  çš„ä¸­ç‡(%)  å›åç‡(%)\n",
      "1.0    85  7761.895768   31  5703.385999   36.47   73.48\n",
      "2.0   235 18255.339929   53 15637.945820   22.55   85.66\n",
      "3.0   307 21470.539609   36 15845.546348   11.73   73.80\n",
      "4.0   324 18697.392648   30 13944.139371    9.26   74.58\n",
      "5.0   375 16944.647486   26 19068.211646    6.93  112.53\n",
      "6.0   284  9610.808403   12  9182.979102    4.23   95.55\n",
      "7.0   180  4085.989764    9 10138.125327    5.00  248.12\n",
      "8.0   100  2173.386392    3  3598.173423    3.00  165.56\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸ å…¨ä½“æ¯”è¼ƒã‚µãƒãƒªãƒ¼\n",
      "--------------------------------------------------\n",
      "      æŒ‡æ¨™  æ—¢å­˜(1ç‚¹è²·) æ–°æˆ¦ç•¥(å¤šç‚¹)\n",
      "  å¯¾è±¡ãƒ¬ãƒ¼ã‚¹æ•°    1084R   1084R\n",
      "  çš„ä¸­ãƒ¬ãƒ¼ã‚¹æ•°     416R    200R\n",
      "çš„ä¸­ç‡(Rå˜ä½)   38.38%  18.45%\n",
      "    ç·æŠ•è³‡é¡ 108,400å†† 99,000å††\n",
      "    ç·æ‰•æˆ»é¡  96,230å†† 93,119å††\n",
      "   æœ€çµ‚å›åç‡   88.77%  94.06%\n",
      "\n",
      "ğŸ’¡ çš„ä¸­æ™‚ã®å¹³å‡åˆæˆã‚ªãƒƒã‚º: 4.66å€\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# ğŸ“Š ã€æ”¹ä¿®ç‰ˆã€‘æˆ¦ç•¥åˆ¥ãƒ»äººæ°—åˆ¥æˆç¸¾ãƒ¬ãƒãƒ¼ãƒˆï¼ˆç‹¬ç«‹å‡ºåŠ›å½¢å¼ï¼‰\n",
    "# ===============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"ğŸ“ˆ æˆ¦ç•¥è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ [è¨­å®š: {BET_STRATEGY} / é™¤å¤–è¨­å®š: {AVOID_NEGATIVE_RETURN}]\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- 1. æ—¢å­˜æˆ¦ç•¥ï¼ˆäºˆæ¸¬1ä½ 1ç‚¹è²·ã„ï¼‰ã®äººæ°—åˆ¥é›†è¨ˆ ---\n",
    "# ãƒ™ãƒ¼ã‚¹ã¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®base_probsã¨åˆã‚ã›ã¦ãã ã•ã„ï¼‰\n",
    "df_base_model = wf_all_xgb.copy() \n",
    "\n",
    "# å„ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬1ä½ã‚’æŠ½å‡º\n",
    "df_single_bet = df_base_model.loc[df_base_model.groupby(\"race_id\")[\"pred_prob_xgb\"].idxmax()].copy()\n",
    "df_single_bet[\"is_hit\"] = (df_single_bet[\"finish_rank\"] == 1).astype(int)\n",
    "df_single_bet[\"payout\"] = df_single_bet[\"is_hit\"] * df_single_bet[\"odds\"] * 100 \n",
    "\n",
    "single_pop_stats = df_single_bet.groupby(\"popularity\").agg(\n",
    "    count=(\"race_id\", \"count\"),\n",
    "    hits=(\"is_hit\", \"sum\"),\n",
    "    payout=(\"payout\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "single_pop_stats[\"çš„ä¸­ç‡(%)\"] = (single_pop_stats[\"hits\"] / single_pop_stats[\"count\"] * 100).round(2)\n",
    "single_pop_stats[\"å›åç‡(%)\"] = (single_pop_stats[\"payout\"] / (single_pop_stats[\"count\"] * 100) * 100).round(2)\n",
    "\n",
    "print(f\"\\nâ–¼ ã€æˆ¦ç•¥â‘ ï¼šæ—¢å­˜ã€‘äºˆæ¸¬1ä½ 1ç‚¹è²·ã„ äººæ°—åˆ¥æˆç¸¾\")\n",
    "display_single = single_pop_stats[single_pop_stats[\"popularity\"] <= 10].copy()\n",
    "display_single.columns = [\"äººæ°—\", \"è³¼å…¥ãƒ¬ãƒ¼ã‚¹æ•°\", \"çš„ä¸­æ•°\", \"æ‰•æˆ»åˆè¨ˆ\", \"çš„ä¸­ç‡(%)\", \"å›åç‡(%)\"]\n",
    "print(display_single.to_string(index=False))\n",
    "\n",
    "\n",
    "# --- 2. æ–°æˆ¦ç•¥ï¼ˆå¤šç‚¹è²·ã„ï¼‰ã®äººæ°—åˆ¥é›†è¨ˆ ---\n",
    "if mc_bet_results:\n",
    "    df_mc_strategy = pd.concat(mc_bet_results)\n",
    "    \n",
    "    mc_pop_stats = df_mc_strategy.groupby(\"popularity\").agg(\n",
    "        bet_count=(\"bet_amount\", \"count\"),\n",
    "        total_bet=(\"bet_amount\", \"sum\"),\n",
    "        hits=(\"payout\", lambda x: (x > 0).sum()),\n",
    "        total_payout=(\"payout\", \"sum\")\n",
    "    ).reset_index()\n",
    "\n",
    "    mc_pop_stats[\"çš„ä¸­ç‡(%)\"] = (mc_pop_stats[\"hits\"] / mc_pop_stats[\"bet_count\"] * 100).round(2)\n",
    "    mc_pop_stats[\"å›åç‡(%)\"] = (mc_pop_stats[\"total_payout\"] / mc_pop_stats[\"total_bet\"] * 100).round(2)\n",
    "\n",
    "    print(f\"\\nâ–¼ ã€æˆ¦ç•¥â‘¡ï¼šæ–°æˆ¦ç•¥ã€‘MCæœŸå¾…å€¤ å¤šç‚¹è²·ã„ äººæ°—åˆ¥æˆç¸¾\")\n",
    "    display_mc = mc_pop_stats[mc_pop_stats[\"popularity\"] <= 10].copy()\n",
    "    display_mc.columns = [\"äººæ°—\", \"è³¼å…¥ç¥¨æ•°\", \"æŠ•è³‡åˆè¨ˆ\", \"çš„ä¸­æ•°\", \"æ‰•æˆ»åˆè¨ˆ\", \"çš„ä¸­ç‡(%)\", \"å›åç‡(%)\"]\n",
    "    print(display_mc.to_string(index=False))\n",
    "\n",
    "    # --- 3. å…¨ä½“æ¯”è¼ƒã‚µãƒãƒªãƒ¼ ---\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"ğŸ å…¨ä½“æ¯”è¼ƒã‚µãƒãƒªãƒ¼\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    total_invest_new = df_mc_strategy[\"bet_amount\"].sum()\n",
    "    total_return_new = df_mc_strategy[\"payout\"].sum()\n",
    "    hit_races_new = df_mc_strategy[df_mc_strategy[\"payout\"] > 0][\"race_id\"].nunique()\n",
    "    \n",
    "    total_invest_old = df_single_bet[\"race_id\"].count() * 100\n",
    "    total_return_old = df_single_bet[\"payout\"].sum()\n",
    "    hit_races_old = df_single_bet[\"is_hit\"].sum()\n",
    "    total_target_races = len(target_races)\n",
    "\n",
    "    summary_data = {\n",
    "        \"æŒ‡æ¨™\": [\"å¯¾è±¡ãƒ¬ãƒ¼ã‚¹æ•°\", \"çš„ä¸­ãƒ¬ãƒ¼ã‚¹æ•°\", \"çš„ä¸­ç‡(Rå˜ä½)\", \"ç·æŠ•è³‡é¡\", \"ç·æ‰•æˆ»é¡\", \"æœ€çµ‚å›åç‡\"],\n",
    "        \"æ—¢å­˜(1ç‚¹è²·)\": [f\"{total_target_races}R\", f\"{hit_races_old}R\", f\"{hit_races_old/total_target_races*100:.2f}%\", f\"{total_invest_old:,.0f}å††\", f\"{total_return_old:,.0f}å††\", f\"{total_return_old/total_invest_old*100:.2f}%\"],\n",
    "        \"æ–°æˆ¦ç•¥(å¤šç‚¹)\": [f\"{total_target_races}R\", f\"{hit_races_new}R\", f\"{hit_races_new/total_target_races*100:.2f}%\", f\"{total_invest_new:,.0f}å††\", f\"{total_return_new:,.0f}å††\", f\"{total_return_new/total_invest_new*100:.2f}%\"]\n",
    "    }\n",
    "    print(pd.DataFrame(summary_data).to_string(index=False))\n",
    "\n",
    "    if BET_STRATEGY == \"dutching\":\n",
    "        race_summary = df_mc_strategy.groupby(\"race_id\").agg(total_bet=(\"bet_amount\", \"sum\"), total_payout=(\"payout\", \"sum\"))\n",
    "        avg_comp_odds = (race_summary[race_summary[\"total_payout\"] > 0][\"total_payout\"] / race_summary[race_summary[\"total_payout\"] > 0][\"total_bet\"]).mean()\n",
    "        print(f\"\\nğŸ’¡ çš„ä¸­æ™‚ã®å¹³å‡åˆæˆã‚ªãƒƒã‚º: {avg_comp_odds:.2f}å€\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nâš  æ–°æˆ¦ç•¥ã§ã®è³¼å…¥ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
