{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af6956c0-b967-49a4-a71d-69829a0c2d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å…¨ãƒ‡ãƒ¼ã‚¿: 634887ä»¶\n",
      "ğŸ‡ éå»å®Ÿç¸¾ãƒ™ãƒ¼ã‚¹ã®è„šè³ªã‚’ç®—å‡ºä¸­...\n",
      "æ¬ æå‡¦ç†ä¸­...\n",
      "å‡¦ç†å¾Œ: 24604ä»¶ï¼ˆå…ƒ:39044ï¼‰\n",
      "ğŸ‡ å±•é–‹äºˆæƒ³ãƒ™ãƒ¼ã‚¹ã®è„šè³ªã‚’ç®—å‡ºä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryo\\AppData\\Local\\Temp\\ipykernel_4228\\3477869052.py:147: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_target[\"running_style_type_pred\"] = df_target.groupby(\"race_id\", group_keys=False).apply(predict_front_back)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¢ ä»Šå›ã®å‡¦ç†ã§ã¯ ã€running_style_type_predã€‘ ã‚’åŸºæº–ã«å‹ç‡ãƒ»å±•é–‹ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
      "âœ… ç‰¹å¾´é‡ä½œæˆå®Œäº†\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "import japanize_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===============================================================\n",
    "# â˜…â˜…â˜… ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šå¤‰æ•° â˜…â˜…â˜…\n",
    "# ===============================================================\n",
    "GROUP_BY_COURSE = False\n",
    "GROUP_BY_NUM_HORSES = True\n",
    "\n",
    "# --- è„šè³ªåˆ¤å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠè¨­å®š ---\n",
    "# \"past\": éå»ã®é€šéé †ä½ç‡ã®å¹³å‡å€¤ã‚’ä½¿ç”¨\n",
    "# \"pred\": ä»Šå›ã®æˆ¦æ³•å(style_name)ã¨æŒ‡æ•°å·®ã«ã‚ˆã‚‹å±•é–‹äºˆæƒ³ã‚’ä½¿ç”¨\n",
    "STYLE_MODE = \"pred\"\n",
    "\n",
    "# ===============================================================\n",
    "# é–¢æ•°å®šç¾©\n",
    "# ===============================================================\n",
    "def confidence_interval(mean: float, std: float, n: int, confidence: float = 0.95):\n",
    "    t_value = stats.t.ppf((1 + confidence) / 2, df=n-1)\n",
    "    margin = t_value * std / np.sqrt(n)\n",
    "    return mean - margin, mean + margin\n",
    "\n",
    "def handle_missing_val(df, features, enabled, min_horses):\n",
    "    if not enabled:\n",
    "        print(\"âš  æ¬ æå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—\")\n",
    "        return df\n",
    "\n",
    "    print(\"æ¬ æå‡¦ç†ä¸­...\")\n",
    "    processed_groups = []\n",
    "    for race_id, group in df.groupby(\"race_id\"):\n",
    "        actual_min_horses = group[\"num_horses\"].iloc[0]\n",
    "        non_missing_mask = group[features].notnull().all(axis=1)\n",
    "        if non_missing_mask.sum() < actual_min_horses: \n",
    "            continue\n",
    "        for col in features:\n",
    "            if group[col].isnull().any():\n",
    "                group[col] = group[col].fillna(group[col].mean(skipna=True))\n",
    "        processed_groups.append(group)\n",
    "    df_processed = pd.concat(processed_groups, ignore_index=True)\n",
    "    print(f\"å‡¦ç†å¾Œ: {len(df_processed)}ä»¶ï¼ˆå…ƒ:{len(df)}ï¼‰\")\n",
    "    return df_processed\n",
    "\n",
    "def get_last_corner_position(row):\n",
    "    for i in [4, 3, 2, 1]:\n",
    "        val = row.get(f\"position_{i}\")\n",
    "        if pd.notnull(val):\n",
    "            return val\n",
    "    return np.nan\n",
    "\n",
    "def create_dl_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid') \n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ===============================================================\n",
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "# ===============================================================\n",
    "# target_num_horses = [8, 9, 10, 11, 12]\n",
    "target_num_horses_int = 8\n",
    "target_num_horses = [target_num_horses_int]\n",
    "target_course_ids = [\n",
    "    102, 103, 201, 202, 301, 302, 402,\n",
    "    501, 502, 601, 602, 700, 702, 802,\n",
    "    900, 901, 1001, 1002, 1201, 1304, 1400, 1402\n",
    "]\n",
    "\n",
    "input_file = \"input_å…¨é ­_ç·¨é›†å¾Œ.csv\"\n",
    "df_all = pd.read_csv(input_file, encoding=\"utf-8\")\n",
    "df_all[\"race_date\"] = pd.to_datetime(df_all[\"race_date\"])\n",
    "print(f\"å…¨ãƒ‡ãƒ¼ã‚¿: {len(df_all)}ä»¶\")\n",
    "\n",
    "# ä¸¦ã³æ›¿ãˆ\n",
    "df_all = df_all.sort_values([\"race_id\", \"horse_number\"])\n",
    "\n",
    "# ===============================================================\n",
    "# A. éå»å®Ÿç¸¾ãƒ™ãƒ¼ã‚¹ã®è„šè³ªç®—å‡º (running_style_type_past)\n",
    "# ===============================================================\n",
    "print(\"ğŸ‡ éå»å®Ÿç¸¾ãƒ™ãƒ¼ã‚¹ã®è„šè³ªã‚’ç®—å‡ºä¸­...\")\n",
    "df_all[\"last_corner_position\"] = df_all.apply(get_last_corner_position, axis=1)\n",
    "df_all[\"last_corner_position_rate\"] = df_all[\"last_corner_position\"] / df_all[\"num_horses\"]\n",
    "\n",
    "# æ™‚ç³»åˆ—é †ã§éå»å¹³å‡ã‚’è¨ˆç®—\n",
    "df_all = df_all.sort_values([\"horse_id\", \"race_date\"])\n",
    "df_all[\"avg_last_corner_pos_rate_past\"] = (\n",
    "    df_all.groupby(\"horse_id\")[\"last_corner_position_rate\"]\n",
    "    .transform(lambda x: x.expanding().mean().shift())\n",
    ")\n",
    "df_all[\"running_style_type_past\"] = np.where(\n",
    "    df_all[\"avg_last_corner_pos_rate_past\"] < 0.5, \"front\",\n",
    "    np.where(df_all[\"avg_last_corner_pos_rate_past\"].notnull(), \"back\", np.nan)\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# å¯¾è±¡ã‚³ãƒ¼ã‚¹Ã—é ­æ•°ã«çµã‚‹ & ç‰¹å¾´é‡åŸºæœ¬ä½œæˆ\n",
    "# ===============================================================\n",
    "df_target = df_all[df_all[\"course_id\"].isin(target_course_ids)].copy()\n",
    "df_target = df_target[df_target[\"num_horses\"].isin(target_num_horses)].copy()\n",
    "\n",
    "handle_missing = True\n",
    "missing_check_features = [\"time_index_average_2\", \"jockey_place_rate_100\"]\n",
    "df_target = handle_missing_val(df_target, features=missing_check_features, enabled=handle_missing, min_horses=target_num_horses_int)\n",
    "\n",
    "# ã‚¿ã‚¤ãƒ æŒ‡æ•°å·®ï¼ˆå±•é–‹äºˆæƒ³ã«ã‚‚ä½¿ã†ãŸã‚ã“ã“ã§ä½œæˆï¼‰\n",
    "race_avg_ti = df_target.groupby(\"race_id\")[\"time_index_average_2\"].transform(\"mean\")\n",
    "df_target[\"time_index_diff_from_avg\"] = df_target[\"time_index_average_2\"] - race_avg_ti\n",
    "\n",
    "# ===============================================================\n",
    "# B. å±•é–‹äºˆæƒ³ãƒ™ãƒ¼ã‚¹ã®è„šè³ªç®—å‡º (running_style_type_pred)\n",
    "# ===============================================================\n",
    "print(\"ğŸ‡ å±•é–‹äºˆæƒ³ãƒ™ãƒ¼ã‚¹ã®è„šè³ªã‚’ç®—å‡ºä¸­...\")\n",
    "style_map = {\"é€ƒã’\": 1, \"å…ˆè¡Œ\": 2, \"å·®ã—\": 3, \"è¿½è¾¼\": 4}\n",
    "df_target[\"style_rank_val\"] = df_target[\"style_name\"].map(style_map).fillna(3)\n",
    "\n",
    "def predict_front_back(group):\n",
    "    # 1. æˆ¦æ³•é †ã€2. æŒ‡æ•°å·®(é«˜ã„ã»ã©å‰) ã§ã‚½ãƒ¼ãƒˆ\n",
    "    group = group.sort_values([\"style_rank_val\", \"time_index_diff_from_avg\"], ascending=[True, False])\n",
    "    n = len(group)\n",
    "    mid = n // 2\n",
    "    res = pd.Series(index=group.index, dtype=str)\n",
    "    res.iloc[:mid] = \"front\"\n",
    "    res.iloc[mid:] = \"back\"\n",
    "    return res\n",
    "\n",
    "df_target[\"running_style_type_pred\"] = df_target.groupby(\"race_id\", group_keys=False).apply(predict_front_back)\n",
    "\n",
    "# ===============================================================\n",
    "# ä½¿ç”¨ã™ã‚‹è„šè³ªã‚«ãƒ©ãƒ ã®åˆ‡ã‚Šæ›¿ãˆè¨­å®š\n",
    "# ===============================================================\n",
    "# STYLE_MODE ã«åŸºã¥ã„ã¦ã€ä»¥é™ã®è¨ˆç®—ã§ä½¿ã†ãƒ¡ã‚¤ãƒ³ã‚«ãƒ©ãƒ ã‚’æ±ºå®š\n",
    "USE_STYLE_COL = f\"running_style_type_{STYLE_MODE}\"\n",
    "print(f\"ğŸ“¢ ä»Šå›ã®å‡¦ç†ã§ã¯ ã€{USE_STYLE_COL}ã€‘ ã‚’åŸºæº–ã«å‹ç‡ãƒ»å±•é–‹ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\")\n",
    "\n",
    "# ===============================================================\n",
    "# ç‰¹å¾´é‡ä½œæˆ (å‹ç‡ãƒ»ç›¸å¯¾æŒ‡æ¨™)\n",
    "# ===============================================================\n",
    "\n",
    "# (1) é¨æ‰‹å‹ç‡å¹³å‡ã¨ã®å·®\n",
    "race_avg_j = df_target.groupby(\"race_id\")[\"jockey_place_rate_100\"].transform(\"mean\")\n",
    "df_target[\"jockey_place_rate_diff_from_avg\"] = df_target[\"jockey_place_rate_100\"] - race_avg_j\n",
    "\n",
    "# (2) é¦¬ç•ªåˆ¥å‹ç‡\n",
    "df_target[\"is_win\"] = (df_target[\"finish_rank\"] == 1).astype(int)\n",
    "win_rate_by_horse_number = (\n",
    "    df_target.groupby([\"course_id\", \"horse_number\"])[\"is_win\"]\n",
    "    .mean().reset_index().rename(columns={\"is_win\": \"win_rate_by_course_horse_number\"})\n",
    ")\n",
    "df_target = df_target.merge(win_rate_by_horse_number, on=[\"course_id\", \"horse_number\"], how=\"left\")\n",
    "race_avg_h = df_target.groupby(\"race_id\")[\"win_rate_by_course_horse_number\"].transform(\"mean\")\n",
    "df_target[\"win_rate_by_course_horse_number_diff_from_avg\"] = df_target[\"win_rate_by_course_horse_number\"] - race_avg_h\n",
    "\n",
    "# (3) é¸æŠã•ã‚ŒãŸè„šè³ªã‚¿ã‚¤ãƒ—åˆ¥ã®ã‚³ãƒ¼ã‚¹å‹ç‡\n",
    "# éå»å…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è„šè³ªå‚¾å‘(overall)ã‚’ãƒãƒ¼ã‚¸ã—ã¦ç®—å‡º\n",
    "horse_mean_style_rate = df_all.groupby(\"horse_id\")[\"last_corner_position_rate\"].mean().reset_index()\n",
    "horse_mean_style_rate[\"running_style_type_overall\"] = np.where(\n",
    "    horse_mean_style_rate[\"last_corner_position_rate\"] < 0.5, \"front\", \"back\"\n",
    ")\n",
    "\n",
    "df_course_all = df_all[df_all[\"course_id\"].isin(target_course_ids)].merge(\n",
    "    horse_mean_style_rate[[\"horse_id\", \"running_style_type_overall\"]], on=\"horse_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "course_style_win_summary = (\n",
    "    df_course_all.groupby([\"course_id\", \"running_style_type_overall\"])\n",
    "    .agg(total_runs=(\"horse_id\", \"count\"), total_wins=(\"finish_rank\", lambda x: (x == 1).sum()))\n",
    "    .reset_index()\n",
    ")\n",
    "course_style_win_summary[\"win_rate_overall\"] = course_style_win_summary[\"total_wins\"] / course_style_win_summary[\"total_runs\"]\n",
    "\n",
    "# é¸æŠã—ãŸè„šè³ª(USE_STYLE_COL)ã«å¯¾ã—ã¦ãƒã‚¹ã‚¿ã‚’ãƒãƒ¼ã‚¸\n",
    "df_target = df_target.merge(\n",
    "    course_style_win_summary[[\"course_id\", \"running_style_type_overall\", \"win_rate_overall\"]],\n",
    "    left_on=[\"course_id\", USE_STYLE_COL],\n",
    "    right_on=[\"course_id\", \"running_style_type_overall\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={\"win_rate_overall\": \"running_style_win_prob\"}).drop(columns=[\"running_style_type_overall\"])\n",
    "\n",
    "race_avg_rs = df_target.groupby(\"race_id\")[\"running_style_win_prob\"].transform(\"mean\")\n",
    "df_target[\"running_style_win_prob_diff_from_avg\"] = df_target[\"running_style_win_prob\"] - race_avg_rs\n",
    "\n",
    "# (4) å±•é–‹ï¼ˆfront/backã®æ¯”ç‡ï¼‰ã«ã‚ˆã‚‹å‹ç‡\n",
    "# é¸æŠã—ãŸè„šè³ªã‚«ãƒ©ãƒ ã«åŸºã¥ã„ã¦ãƒ¬ãƒ¼ã‚¹å†…ã®æ¯”ç‡ã‚’è¨ˆç®—\n",
    "race_style_counts = df_target.groupby([\"race_id\", USE_STYLE_COL]).size().unstack(fill_value=0).reset_index()\n",
    "for col in [\"front\", \"back\"]:\n",
    "    if col not in race_style_counts.columns: race_style_counts[col] = 0\n",
    "\n",
    "race_style_counts[\"total\"] = race_style_counts[\"front\"] + race_style_counts[\"back\"]\n",
    "race_style_counts[\"ratio_front_round\"] = (race_style_counts[\"front\"] / race_style_counts[\"total\"]).round(2)\n",
    "race_style_counts[\"ratio_back_round\"] = (race_style_counts[\"back\"] / race_style_counts[\"total\"]).round(2)\n",
    "\n",
    "df_target = df_target.merge(\n",
    "    race_style_counts[[\"race_id\", \"ratio_front_round\", \"ratio_back_round\"]], on=\"race_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# å±•é–‹åˆ¥å‹ç‡ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿å†…ã§è¨ˆç®—ï¼‰\n",
    "style_win_ratio_df = df_target.groupby([\n",
    "    \"course_id\", USE_STYLE_COL, \"ratio_front_round\", \"ratio_back_round\"\n",
    "]).agg(total_horses=(\"race_id\", \"count\"), total_wins=(\"is_win\", \"sum\")).reset_index()\n",
    "style_win_ratio_df[\"win_rate\"] = style_win_ratio_df[\"total_wins\"] / style_win_ratio_df[\"total_horses\"]\n",
    "\n",
    "df_target = df_target.merge(\n",
    "    style_win_ratio_df[[\"course_id\", USE_STYLE_COL, \"ratio_front_round\", \"ratio_back_round\", \"win_rate\"]],\n",
    "    on=[\"course_id\", USE_STYLE_COL, \"ratio_front_round\", \"ratio_back_round\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={\"win_rate\": \"style_win_prob_by_ratio\"})\n",
    "\n",
    "race_avg_ratio = df_target.groupby(\"race_id\")[\"style_win_prob_by_ratio\"].transform(\"mean\")\n",
    "df_target[\"style_win_prob_by_ratio_diff_from_avg\"] = df_target[\"style_win_prob_by_ratio\"] - race_avg_ratio\n",
    "\n",
    "print(\"âœ… ç‰¹å¾´é‡ä½œæˆå®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "baf05259-9480-481a-b7cf-eb5c17d7f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å³é¸å®Œäº†ï¼šç´”ç²‹ãª8é ­ç«‹ã¦ãƒ¬ãƒ¼ã‚¹ã®ã¿ã€è¨ˆ 2582 ãƒ¬ãƒ¼ã‚¹ã«çµã‚Šè¾¼ã‚“ã ã´ã‚‡ï¼\n"
     ]
    }
   ],
   "source": [
    "# 1. å„ãƒ¬ãƒ¼ã‚¹ã«ä½•è¡Œï¼ˆä½•é ­ï¼‰ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹\n",
    "race_counts = df_target.groupby(\"race_id\").size()\n",
    "\n",
    "# 2. ã€Œãƒ‡ãƒ¼ã‚¿ã®è¡Œæ•°ã€ãŒ target_num_horses_int (8) ã¨ä¸€è‡´ã—ã€\n",
    "#    ã‹ã¤ã€Œå…¬ç§°ã®é ­æ•°(num_horses)ã€ã‚‚ 8 ã§ã‚ã‚‹ãƒ¬ãƒ¼ã‚¹IDã‚’ç‰¹å®šã™ã‚‹\n",
    "valid_race_ids = race_counts[\n",
    "    (race_counts == target_num_horses_int) & \n",
    "    (race_counts.index.map(df_target.groupby(\"race_id\")[\"num_horses\"].first()) == target_num_horses_int)\n",
    "].index\n",
    "\n",
    "# 3. ãã‚Œã‚‰ã®ãƒ¬ãƒ¼ã‚¹IDã ã‘ã‚’æŠœãå‡ºã™\n",
    "df_target = df_target[df_target[\"race_id\"].isin(valid_race_ids)].copy()\n",
    "\n",
    "print(f\"âœ… å³é¸å®Œäº†ï¼šç´”ç²‹ãª{target_num_horses_int}é ­ç«‹ã¦ãƒ¬ãƒ¼ã‚¹ã®ã¿ã€è¨ˆ {df_target['race_id'].nunique()} ãƒ¬ãƒ¼ã‚¹ã«çµã‚Šè¾¼ã‚“ã ã´ã‚‡ï¼\")\n",
    "\n",
    "df_target.to_csv('C:\\\\Users\\\\ryo\\\\Downloads\\\\df_target.csv', index=False, encoding='cp932')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbfceaa5-37e1-4829-8f58-7a77f6a45c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š å±•é–‹äºˆæƒ³ã®æ­£ç­”ç‡ï¼ˆç­”ãˆåˆã‚ã›ï¼‰ã‚’ç®—å‡ºä¸­...\n",
      "------------------------------\n",
      "âœ… å…¨ä½“æ­£ç­”ç‡: 68.59%\n",
      "------------------------------\n",
      "â–  è„šè³ª(style_name)åˆ¥ã®äºˆæƒ³æ­£ç­”ç‡:\n",
      "style_name\n",
      "é€ƒã’    80.06%\n",
      "è¿½è¾¼    75.97%\n",
      "å…ˆè¡Œ    75.72%\n",
      "å·®ã—    63.49%\n",
      "Name: is_correct, dtype: object\n",
      "------------------------------\n",
      "â–  ã‚³ãƒ¼ã‚¹åˆ¥æ­£ç­”ç‡ï¼ˆãƒ¯ãƒ¼ã‚¹ãƒˆ5ï¼‰: â€»æ”¹å–„ã®ãƒ’ãƒ³ãƒˆã«ãªã‚Šã¾ã™\n",
      "course_id\n",
      "702     61.54%\n",
      "1201    62.50%\n",
      "1304    63.75%\n",
      "802     64.29%\n",
      "700     65.53%\n",
      "Name: is_correct, dtype: object\n",
      "------------------------------\n",
      "â–  äºˆæ¸¬ã®å‚¾å‘ï¼ˆè¡Œï¼šå®Ÿéš› / åˆ—ï¼šäºˆæ¸¬ï¼‰\n",
      "running_style_type_pred    back   front\n",
      "actual_style                           \n",
      "back                     68.56%  31.44%\n",
      "front                    31.38%  68.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryo\\AppData\\Local\\Temp\\ipykernel_4228\\265104599.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  eval_df[\"actual_style\"] = eval_df.groupby(\"race_id\", group_keys=False).apply(label_actual_front_back)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# å±•é–‹äºˆæƒ³ (running_style_type_pred) ã®ç­”ãˆåˆã‚ã›\n",
    "# ===============================================================\n",
    "print(\"\\nğŸ“Š å±•é–‹äºˆæƒ³ã®æ­£ç­”ç‡ï¼ˆç­”ãˆåˆã‚ã›ï¼‰ã‚’ç®—å‡ºä¸­...\")\n",
    "\n",
    "def evaluate_style_prediction(df):\n",
    "    # 1. æœ€çµ‚ã‚³ãƒ¼ãƒŠãƒ¼é †ä½ã®å–å¾— (æ—¢å­˜ã® get_last_corner_position ã‚’æ´»ç”¨)\n",
    "    # â€» get_last_corner_position ã¯ position_4, 3, 2, 1 ã®é †ã«æœ€åˆã«è¦‹ã¤ã‹ã£ãŸéæ¬ æå€¤ã‚’è¿”ã™\n",
    "    df[\"actual_last_corner_pos\"] = df.apply(get_last_corner_position, axis=1)\n",
    "    \n",
    "    # æœ€çµ‚ã‚³ãƒ¼ãƒŠãƒ¼é †ä½ãŒå–å¾—ã§ããªã„ãƒ‡ãƒ¼ã‚¿ã¯è©•ä¾¡ã‹ã‚‰é™¤å¤–\n",
    "    eval_df = df[df[\"actual_last_corner_pos\"].notnull()].copy()\n",
    "    \n",
    "    def label_actual_front_back(group):\n",
    "        # ãƒ¬ãƒ¼ã‚¹å†…ã§ã®æœ€çµ‚ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½ã«åŸºã¥ã„ã¦ã€å®Ÿéš›ã®çµæœ(actual_style)ã‚’åˆ¤å®š\n",
    "        # ã“ã“ã§ã‚‚äºˆæ¸¬æ™‚ã¨åŒã˜ãã€Œæ˜‡é †ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½åŠåˆ†ã€ã‚’ front ã¨ã™ã‚‹\n",
    "        group = group.sort_values(\"actual_last_corner_pos\", ascending=True)\n",
    "        n = len(group)\n",
    "        mid = n // 2\n",
    "        \n",
    "        res = pd.Series(index=group.index, dtype=str)\n",
    "        res.iloc[:mid] = \"front\"\n",
    "        res.iloc[mid:] = \"back\"\n",
    "        return res\n",
    "\n",
    "    # å®Ÿéš›ã®å±•é–‹çµæœã‚’ãƒ©ãƒ™ãƒ«ä»˜ã‘\n",
    "    eval_df[\"actual_style\"] = eval_df.groupby(\"race_id\", group_keys=False).apply(label_actual_front_back)\n",
    "    \n",
    "    # 2. äºˆæ¸¬ã¨å®Ÿç¸¾ã®æ¯”è¼ƒ\n",
    "    # äºˆæ¸¬(running_style_type_pred) ã¨ å®Ÿç¸¾(actual_style) ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹\n",
    "    eval_df[\"is_correct\"] = (eval_df[\"running_style_type_pred\"] == eval_df[\"actual_style\"])\n",
    "    \n",
    "    # 3. å…¨ä½“æ­£ç­”ç‡\n",
    "    overall_accuracy = eval_df[\"is_correct\"].mean()\n",
    "    \n",
    "    # 4. è„šè³ª(style_name)ã”ã¨ã®æ­£ç­”ç‡\n",
    "    style_accuracy = eval_df.groupby(\"style_name\")[\"is_correct\"].mean().sort_values(ascending=False)\n",
    "    \n",
    "    # 5. ã‚³ãƒ¼ã‚¹ã”ã¨ã®æ­£ç­”ç‡ï¼ˆä¸Šä½5ãƒ»ä¸‹ä½5ï¼‰\n",
    "    course_accuracy = eval_df.groupby(\"course_id\")[\"is_correct\"].mean()\n",
    "    \n",
    "    return overall_accuracy, style_accuracy, course_accuracy, eval_df\n",
    "\n",
    "# å®Ÿè¡Œ\n",
    "accuracy, style_acc, course_acc, result_df = evaluate_style_prediction(df_target)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"âœ… å…¨ä½“æ­£ç­”ç‡: {accuracy:.2%}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"â–  è„šè³ª(style_name)åˆ¥ã®äºˆæƒ³æ­£ç­”ç‡:\")\n",
    "print(style_acc.map(lambda x: f\"{x:.2%}\"))\n",
    "print(\"-\" * 30)\n",
    "print(\"â–  ã‚³ãƒ¼ã‚¹åˆ¥æ­£ç­”ç‡ï¼ˆãƒ¯ãƒ¼ã‚¹ãƒˆ5ï¼‰: â€»æ”¹å–„ã®ãƒ’ãƒ³ãƒˆã«ãªã‚Šã¾ã™\")\n",
    "print(course_acc.sort_values().head(5).map(lambda x: f\"{x:.2%}\"))\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# æ··åŒè¡Œåˆ—ï¼ˆãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼‰ã®è¡¨ç¤º\n",
    "confusion_matrix = pd.crosstab(result_df['actual_style'], result_df['running_style_type_pred'], normalize='index')\n",
    "print(\"â–  äºˆæ¸¬ã®å‚¾å‘ï¼ˆè¡Œï¼šå®Ÿéš› / åˆ—ï¼šäºˆæ¸¬ï¼‰\")\n",
    "print(confusion_matrix.map(lambda x: f\"{x:.2%}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94f4a123-c251-4ebd-9e92-717151fe69c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ãƒ‡ãƒ¼ã‚¿æ•´å½¢ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2582/2582 [00:14<00:00, 175.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¥ äºˆæ¸¬å®Œäº†ã ã´ã‚‡ï¼\n",
      "1ç€ç¢ºç‡:\n",
      "[0.06421342 0.13377984 0.07168897 0.1503606  0.07781889 0.16019955\n",
      " 0.15074672 0.19119203]\n",
      "ã‚·ãƒŠãƒªã‚ªç¢ºç‡ [å‰, å·®]: [0.00165056 0.9983494 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- è¨­å®š ---\n",
    "num_feat_cols = [\n",
    "    \"time_index_diff_from_avg\", \n",
    "#    \"jockey_place_rate_diff_from_avg\",\n",
    "#    \"win_rate_by_course_horse_number_diff_from_avg\",\n",
    "#    \"running_style_win_prob_diff_from_avg\",\n",
    "#    \"style_win_prob_by_ratio_diff_from_avg\"\n",
    "]\n",
    "cat_feat_cols = [\"horse_id\", \"jockey_id\", \"horse_number\", \"running_style_type_pred\"]\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "# æˆ»ã‚Šå€¤: æ•°å€¤ãƒ†ãƒ³ã‚½ãƒ«, ã‚«ãƒ†ã‚´ãƒªãƒ†ãƒ³ã‚½ãƒ«(IDãŒ4ã¤ä¸¦ã‚“ã§ã‚‹), æ­£è§£ãƒ©ãƒ™ãƒ«\n",
    "X_num, X_cat, y_tensor = prepare_race_data(df_target, num_feat_cols, cat_feat_cols)\n",
    "\n",
    "# å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°ï¼ˆEmbeddingå±¤ã®ã‚µã‚¤ã‚ºï¼‰ã‚’å–å¾—\n",
    "# X_catã¯ (ãƒ¬ãƒ¼ã‚¹, 8é ­, 4ç¨®é¡) ã®å½¢ã€‚å„ç¨®é¡ã®æœ€å¤§å€¤+1ã‚’ã‚µã‚¤ã‚ºã«ã™ã‚‹ã´ã‚‡\n",
    "cat_dims = [X_cat[:,:,i].max().item() + 1 for i in range(len(cat_feat_cols))]\n",
    "\n",
    "class MultiEmbedScenarioModel(nn.Module):\n",
    "    def __init__(self, num_dim, cat_dims):\n",
    "        super().__init__()\n",
    "        # å„Embeddingå±¤ã®å®šç¾©\n",
    "        self.horse_emb = nn.Embedding(cat_dims[0], 16)\n",
    "        self.jockey_emb = nn.Embedding(cat_dims[1], 8)\n",
    "        self.num_emb   = nn.Embedding(cat_dims[2], 4)\n",
    "        self.style_emb = nn.Embedding(cat_dims[3], 2)\n",
    "        \n",
    "        # åˆè¨ˆæ¬¡å…ƒæ•°: æ•°å€¤(5) + åŸ‹ã‚è¾¼ã¿(16+8+4+2) = 35\n",
    "        # å‰²ã‚Šåˆ‡ã‚Œã‚‹ã‚ˆã†ã«èª¿æ•´ï¼ˆä¾‹: 35 -> 36ã«ã™ã‚‹ãŸã‚ã«ã€æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®æ‰±ã„ã‚’èª¿æ•´ or åŸ‹ã‚è¾¼ã¿ã‚’èª¿æ•´ï¼‰\n",
    "        # ä»Šå›ã¯ total_dim=36 ã«ãªã‚‹ã‚ˆã†ã« style_embã‚’3æ¬¡å…ƒã«ã™ã‚‹ã´ã‚‡ï¼\n",
    "        self.style_emb = nn.Embedding(cat_dims[3], 3)\n",
    "        self.total_dim = num_dim + 16 + 8 + 4 + 3 # = 36\n",
    "        \n",
    "        # 36ãªã‚‰ num_heads=4 ã§å‰²ã‚Šåˆ‡ã‚Œã‚‹ã´ã‚‡ï¼\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=self.total_dim, num_heads=4, batch_first=True)\n",
    "        \n",
    "        self.head_scenario_A = nn.Linear(self.total_dim, 1)\n",
    "        self.head_scenario_B = nn.Linear(self.total_dim, 1)\n",
    "        self.gate = nn.Linear(self.total_dim * target_num_horses_int, 2)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        # x_catã®å„åˆ—ã‚’å–ã‚Šå‡ºã—ã¦åŸ‹ã‚è¾¼ã‚€\n",
    "        h_e = self.horse_emb(x_cat[:, :, 0])\n",
    "        j_e = self.jockey_emb(x_cat[:, :, 1])\n",
    "        n_e = self.num_emb(x_cat[:, :, 2])\n",
    "        s_e = self.style_emb(x_cat[:, :, 3])\n",
    "        \n",
    "        # å…¨éƒ¨ãã£ã¤ã‘ã‚‹ã´ã‚‡ï¼ (æ•°å€¤ + å…¨åŸ‹ã‚è¾¼ã¿)\n",
    "        combined = torch.cat([x_num, h_e, j_e, n_e, s_e], dim=-1)\n",
    "        \n",
    "        # 1. å±•é–‹ã®ç›¸äº’ä½œç”¨\n",
    "        attn_out, _ = self.attention(combined, combined, combined)\n",
    "        \n",
    "        # 2. ã‚·ãƒŠãƒªã‚ªåˆ¥ã‚¹ã‚³ã‚¢\n",
    "        score_A = self.head_scenario_A(attn_out).squeeze(-1)\n",
    "        score_B = self.head_scenario_B(attn_out).squeeze(-1)\n",
    "        \n",
    "        # 3. ã‚²ãƒ¼ãƒˆï¼ˆå±•é–‹äºˆæ¸¬ï¼‰\n",
    "        gate_out = self.gate(combined.view(combined.size(0), -1))\n",
    "        scenario_weights = F.softmax(gate_out, dim=-1)\n",
    "        \n",
    "        # 4. åˆæˆ\n",
    "        prob_A = F.softmax(score_A, dim=-1)\n",
    "        prob_B = F.softmax(score_B, dim=-1)\n",
    "        final_probs = scenario_weights[:, 0:1] * prob_A + scenario_weights[:, 1:2] * prob_B\n",
    "        \n",
    "        return final_probs, scenario_weights\n",
    "\n",
    "# --- ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ– ---\n",
    "model = MultiEmbedScenarioModel(len(num_feat_cols), cat_dims)\n",
    "\n",
    "# --- è©¦é‹è»¢ ---\n",
    "probs, scenario_weights = model(X_num[:5], X_cat[:5])\n",
    "print(\"\\nğŸ”¥ äºˆæ¸¬å®Œäº†ã ã´ã‚‡ï¼\")\n",
    "print(f\"1ç€ç¢ºç‡:\\n{probs[0].detach().numpy()}\")\n",
    "print(f\"ã‚·ãƒŠãƒªã‚ªç¢ºç‡ [å‰, å·®]: {scenario_weights[0].detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67fc0e9c-5e85-42d4-a21c-b33c6239d398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã€‘äººæ°—åˆ¥æˆç¸¾\n",
      "|   popularity |   æ¡ç”¨é–¾å€¤ |   è³¼å…¥é¦¬åˆ¸æ•° |   çš„ä¸­æ•° | çš„ä¸­ç‡   | å›åç‡   |   ç·æŠ•è³‡é¡ |   ç·å›åé¡ |\n",
      "|-------------:|-----------:|-------------:|---------:|:---------|:---------|-----------:|-----------:|\n",
      "|            2 |        1.5 |           19 |        2 | 10.5%    | 98.4%    |       1900 |       1870 |\n",
      "|            3 |        2   |           99 |        4 | 4.0%     | 62.3%    |       9900 |       6170 |\n",
      "|            4 |        2   |          452 |        9 | 2.0%     | 37.4%    |      45200 |      16900 |\n",
      "|            5 |        2   |         1078 |       31 | 2.9%     | 74.7%    |     107800 |      80560 |\n",
      "|            6 |        2   |         1688 |       47 | 2.8%     | 80.7%    |     168800 |     136250 |\n",
      "|            7 |        2   |         1983 |       39 | 2.0%     | 83.2%    |     198300 |     165080 |\n",
      "|            8 |        2   |         2059 |       10 | 0.5%     | 36.8%    |     205900 |      75780 |\n",
      "\n",
      "âœ¨ å…¨ä½“å›åç‡: 65.4%\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 1. ãƒ‡ãƒ¼ã‚¿ã®ä»•åˆ†ã‘ï¼ˆNã‚’æŒ‡å®šï¼‰\n",
    "# ===============================================================\n",
    "N = 500  # ã“ã“ã«å­¦ç¿’ã«ä½¿ã„ãŸã„ãƒ¬ãƒ¼ã‚¹æ•°ã‚’æŒ‡å®šã™ã‚‹ã´ã‚‡ï¼\n",
    "\n",
    "# ãƒ†ãƒ³ã‚½ãƒ«ã‚’åˆ†å‰²\n",
    "X_num_train, X_num_test = X_num[:N], X_num[N:]\n",
    "X_cat_train, X_cat_test = X_cat[:N], X_cat[N:]\n",
    "y_train, y_test = y_tensor[:N], y_tensor[N:]\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®å…ƒã®ãƒ‡ãƒ¼ã‚¿\n",
    "valid_race_ids = df_target[df_target['num_horses'] == target_num_horses_int]['race_id'].unique()\n",
    "test_race_ids = valid_race_ids[N:]\n",
    "df_test_raw = df_target[df_target['race_id'].isin(test_race_ids)].sort_values(['race_id', 'horse_number']).copy()\n",
    "\n",
    "# ===============================================================\n",
    "# 2. å­¦ç¿’ã®å®Ÿè¡Œï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰\n",
    "# ===============================================================\n",
    "model.train()\n",
    "train_loader = DataLoader(TensorDataset(X_num_train, X_cat_train, y_train), batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(30):\n",
    "    for b_num, b_cat, b_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(b_num, b_cat)\n",
    "        loss = criterion(outputs, b_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# ===============================================================\n",
    "# 3. äºˆæƒ³ã¨é¦¬åˆ¸è³¼å…¥ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆäººæ°—åˆ¥é–¾å€¤ã‚’å°å…¥ï¼ï¼‰\n",
    "# ===============================================================\n",
    "# äººæ°—ï¼ˆpopularityï¼‰ã”ã¨ã®æœŸå¾…å€¤é–¾å€¤ã‚’è¨­å®šã™ã‚‹ã´ã‚‡ï¼\n",
    "# æ•°å­—ã‚’æ›¸ãæ›ãˆã¦æˆ¦ç•¥ã‚’ç·´ã£ã¦ã»ã—ã„ã´ã‚‡ã€‚\n",
    "thresholds = {\n",
    "    1.0: 1.20,\n",
    "    2.0: 1.50,\n",
    "    3.0: 2.00,\n",
    "    4.0: 2.00,\n",
    "}\n",
    "default_threshold = 2.00\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_probs, _ = model(X_num_test, X_cat_test)\n",
    "    pred_probs = pred_probs.numpy().flatten()\n",
    "\n",
    "df_test_raw['pred_prob'] = pred_probs\n",
    "df_test_raw['expected_value'] = df_test_raw['pred_prob'] * df_test_raw['odds']\n",
    "\n",
    "# äººæ°—ã”ã¨ã®é–¾å€¤ã‚’é©ç”¨ã™ã‚‹ã´ã‚‡\n",
    "df_test_raw['threshold'] = df_test_raw['popularity'].map(thresholds).fillna(default_threshold)\n",
    "\n",
    "# å„é¦¬ã®æœŸå¾…å€¤ãŒã€ãã®äººæ°—ã®é–¾å€¤ã‚’è¶…ãˆã¦ã„ã‚‹ã‹åˆ¤å®š\n",
    "df_test_raw['bet_amount'] = np.where(df_test_raw['expected_value'] > df_test_raw['threshold'], 100, 0)\n",
    "\n",
    "# çš„ä¸­ã—ãŸæ™‚ã®æ‰•ã„æˆ»ã—\n",
    "df_test_raw['return_amount'] = np.where((df_test_raw['bet_amount'] > 0) & (df_test_raw['is_win'] == 1), \n",
    "                                        df_test_raw['odds'] * 100, 0)\n",
    "\n",
    "# ===============================================================\n",
    "# 4. äººæ°—é †ï¼ˆpopularityï¼‰åˆ¥ã®é›†è¨ˆï¼ˆçš„ä¸­æ•°ã‚’è¿½åŠ ï¼ï¼‰\n",
    "# ===============================================================\n",
    "summary = df_test_raw[df_test_raw['bet_amount'] > 0].groupby('popularity').agg(\n",
    "    è³¼å…¥ãƒ¬ãƒ¼ã‚¹æ•°=('race_id', 'nunique'),\n",
    "    è³¼å…¥é¦¬åˆ¸æ•°=('bet_amount', 'count'),\n",
    "    çš„ä¸­æ•°=('is_win', 'sum'), # çš„ä¸­æ•°ã‚’è¿½åŠ ã—ãŸã´ã‚‡ï¼\n",
    "    ç·æŠ•è³‡é¡=('bet_amount', 'sum'),\n",
    "    ç·å›åé¡=('return_amount', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "summary['çš„ä¸­ç‡'] = (summary['çš„ä¸­æ•°'] / summary['è³¼å…¥é¦¬åˆ¸æ•°'] * 100).round(1).astype(str) + '%'\n",
    "summary['å›åç‡'] = (summary['ç·å›åé¡'] / summary['ç·æŠ•è³‡é¡'] * 100).round(1).astype(str) + '%'\n",
    "summary['æ¡ç”¨é–¾å€¤'] = summary['popularity'].map(thresholds).fillna(default_threshold)\n",
    "\n",
    "print(\"\\nğŸ“Š ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã€‘äººæ°—åˆ¥æˆç¸¾\")\n",
    "# çš„ä¸­æ•°ã¨æ¡ç”¨é–¾å€¤ã‚’è¡¨ç¤ºé …ç›®ã«åŠ ãˆãŸã´ã‚‡\n",
    "display_cols = ['popularity', 'æ¡ç”¨é–¾å€¤', 'è³¼å…¥é¦¬åˆ¸æ•°', 'çš„ä¸­æ•°', 'çš„ä¸­ç‡', 'å›åç‡', 'ç·æŠ•è³‡é¡', 'ç·å›åé¡']\n",
    "print(summary[display_cols].to_markdown(index=False))\n",
    "\n",
    "# å…¨ä½“æˆç¸¾\n",
    "total_invest = summary['ç·æŠ•è³‡é¡'].sum()\n",
    "total_return = summary['ç·å›åé¡'].sum()\n",
    "print(f\"\\nâœ¨ å…¨ä½“å›åç‡: {total_return / total_invest:.1%}\" if total_invest > 0 else \"\\nè³¼å…¥æ¡ä»¶ã«åˆã†é¦¬ãŒã„ãªã‹ã£ãŸã´ã‚‡...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
